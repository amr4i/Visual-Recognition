{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "1. You have to use only this notebook for all your code.\n",
    "2. All the results and plots should be mentioned in this notebook.\n",
    "3. For final submission, submit this notebook along with the report ( usual 2-4 pages, latex typeset, which includes the challenges faces and details of additional steps, if any)\n",
    "4. Marking scheme\n",
    "    -  **60%**: Your code should be able to detect bounding boxes using resnet 18, correct data loading and preprocessing. Plot any 5 correct and 5 incorrect sample detections from the test set in this notebook for both the approached (1 layer and 2 layer detection), so total of 20 plots.\n",
    "    -  **20%**: Use two layers (multi-scale feature maps) to detect objects independently as in SSD (https://arxiv.org/abs/1512.02325).  In this method, 1st detection will be through the last layer of Resnet18 and the 2nd detection could be through any layer before the last layer. SSD uses lower resolution layers to detect larger scale objects. \n",
    "    -  **20%**: Implement Non-maximum suppression (NMS) (should not be imported from any library) on the candidate bounding boxes.\n",
    "    \n",
    "5. Report AP for each of the three class and mAP score for the complete test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1.post2\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "%matplotlib inline\n",
    "plt.ion()\n",
    "# Import other modules if required\n",
    "# Can use other libraries as well\n",
    "\n",
    "print(torch.__version__)\n",
    "resnet_input = 224 #size of resnet18 input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your hyper-parameters using validation data\n",
    "batch_size = 128\n",
    "num_epochs = 5\n",
    "learning_rate =  0.001\n",
    "hyp_momentum = 0.9\n",
    "validation_split = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build the data\n",
    "Use the following links to locally download the data:\n",
    "<br/>Training and validation:\n",
    "<br/>http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "<br/>Testing data:\n",
    "<br/>http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
    "<br/>The dataset consists of images from 20 classes, with detection annotations included. The JPEGImages folder houses the images, and the Annotations folder has the object-wise labels for the objects in one xml file per image. You have to extract the object information, i.e. the [xmin, ymin] (the top left x,y co-ordinates) and the [xmax, ymax] (the bottom right x,y co-ordinates) of only the objects belonging to the three classes(aeroplane, bottle, chair). For parsing the xml file, you can import xml.etree.ElementTree for you. <br/>\n",
    "<br/> Organize the data as follows:\n",
    "<br/> For every image in the dataset, extract/crop the object patch from the image one by one using their respective co-ordinates:[xmin, ymin, xmax, ymax], resize the image to resnet_input, and store it with its class label information. Do the same for training/validation and test datasets. <br/>\n",
    "##### Important\n",
    "You also have to collect data for an extra background class which stands for the class of an object which is not a part of any of the 20 classes. For this, you can crop and resize any random patches from an image. A good idea is to extract patches that have low \"intersection over union\" with any object present in the image frame from the 20 Pascal VOC classes. The number of background images should be roughly around those of other class objects' images. Hence the total classes turn out to be four. This is important for applying the sliding window method later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('__background__',\n",
    "           'aeroplane',\n",
    "           'bottle',\n",
    "           'chair'\n",
    "           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou(boxA, boxB):\n",
    "    # print(boxA, boxB)\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0.0, xB - xA + 1.0) * max(0.0, yB - yA + 1.0)\n",
    "\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1.0) * (boxA[3] - boxA[1] + 1.0)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1.0) * (boxB[3] - boxB[1] + 1.0)\n",
    "\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = 1.0*interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "    # return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class voc_dataset(torch.utils.data.Dataset): # Extend PyTorch's Dataset class\n",
    "    def __init__(self, root_dir, train, transform=None):\n",
    "        # Begin\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.allowed_labels = ['aeroplane', 'bottle', 'chair']\n",
    "        self.dataset = []\n",
    "        self.train = train\n",
    "        self.make_dataset()\n",
    "    \n",
    "    def make_dataset(self):\n",
    "        images_folder = os.path.join(self.root_dir, 'VOC2007/JPEGImages/')\n",
    "        objects_folder = os.path.join(self.root_dir, 'VOC2007/Annotations/')\n",
    "        bg_entries = []\n",
    "        for image_name in os.listdir(images_folder):\n",
    "            # print(image_name)\n",
    "            data_coords = []\n",
    "            image_file = os.path.join(images_folder, image_name)\n",
    "            objects_file = os.path.join(objects_folder, image_name.split(\".\")[0]+'.xml')\n",
    "            tree = ET.parse(objects_file)\n",
    "            # img = cv2.imread(image_file)\n",
    "            img = Image.open(image_file).convert('RGB')\n",
    "            root = tree.getroot()\n",
    "            # iterate over all objects in the image\n",
    "            for obj in root.findall('object'):\n",
    "                label = obj.find('name').text\n",
    "                # skip this object if it isnt one of the required ones\n",
    "                if label not in self.allowed_labels:\n",
    "                    continue\n",
    "                bndbox = obj.find('bndbox')\n",
    "                coord = []\n",
    "                for cd in bndbox:\n",
    "                    coord.append(int(cd.text))\n",
    "                # coord is in order: xmin, ymin, xmax, ymax\n",
    "                \n",
    "                # exract the object image from the complete image\n",
    "                object_img = img.crop(tuple(coord))\n",
    "                if self.transform:\n",
    "                    object_img = self.transform(object_img)\n",
    "                \n",
    "                # save the object and the its label in our dataset\n",
    "                label_vec = np.array([i==classes.index(label) for i in range(4)]).astype(int)\n",
    "                data_entry = {'image': object_img, 'label':classes.index(label)}\n",
    "                self.dataset.append(data_entry)\n",
    "                data_coords.append(coord)\n",
    "                # print(label)\n",
    "            \n",
    "            # get random backgrounds from the image\n",
    "            width = int(root.find('./size/width').text)\n",
    "            height = int(root.find('./size/height').text)\n",
    "            maxTries = 4\n",
    "            try:\n",
    "                while(maxTries>0):\n",
    "                    maxTries -= 1\n",
    "                    isValidBG = True\n",
    "                    x1 = random.randint(0, width-resnet_input-1)\n",
    "                    y1 = random.randint(0, height-resnet_input-1)\n",
    "                    x2, y2 = x1+resnet_input-1, y1+resnet_input-1\n",
    "                    bg_image = img.crop((x1, y1, x2, y2))\n",
    "                    if self.transform:\n",
    "                        bg_image = self.transform(bg_image)\n",
    "                    # check for IoU match of the backgroud with extracted object images \n",
    "                    for data_c in data_coords:\n",
    "                        if get_iou([x1,y1,x2,y2], data_c) >= 0.5:\n",
    "                            isValidBG = False\n",
    "                            break\n",
    "                    if isValidBG == False:\n",
    "                        continue\n",
    "                    bg_label = '__background__'\n",
    "                    label_vec = np.array([i==classes.index(bg_label) for i in range(4)]).astype(int)\n",
    "                    bg_entries.append({'image':bg_image, 'label':classes.index(label)})\n",
    "                    break\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        # randomly pick the required number of background images\n",
    "        random.shuffle(bg_entries)\n",
    "        numToAdd = int(len(self.dataset)/len(self.allowed_labels))+1\n",
    "        self.dataset = self.dataset + bg_entries[:numToAdd]\n",
    "        random.shuffle(self.dataset)\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Begin\n",
    "        return len(self.dataset)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "       # Begin\n",
    "        return self.dataset[idx]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset():\n",
    "    # Begin\n",
    "    dataset = voc_dataset('./trial', 0)\n",
    "    print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# to run the build dataset funcion\n",
    "build_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the netwok\n",
    "<br/>You can train the network on the created dataset. This will yield a classification network on the 4 classes of the VOC dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "composed_transform = transforms.Compose([transforms.Resize((resnet_input,resnet_input)),\n",
    "                                         transforms.RandomHorizontalFlip(),\n",
    "                                         transforms.ToTensor(),])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = voc_dataset(root_dir='./train', train=True, transform=composed_transform) # Supply proper root_dir\n",
    "test_dataset = voc_dataset(root_dir='./test', train=False, transform=composed_transform) # Supply proper root_dir\n",
    "\n",
    "# to split training in train and val sets\n",
    "dataset_size = len(train_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "val_size = int(np.floor(validation_split * dataset_size))\n",
    "train_indices, val_indices = indices[val_size:], indices[:val_size]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "tr_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)\n",
    "\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "data_loaders = {'train': tr_loader, 'val':val_loader, 'test':test_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "Use the pre-trained network to fine-tune the network in the following section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "resnet18.fc = torch.nn.Linear(resnet18.fc.in_features, 4)\n",
    "\n",
    "print(resnet18)\n",
    "# Add code for using CUDA here\n",
    "# CUDA has not been used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# Update if any errors occur\n",
    "optimizer = torch.optim.SGD(resnet18.parameters(), learning_rate, hyp_momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#One Layer Detection\n",
    "def train(model, dataloaders, criterion, optimizer, num_epochs):\n",
    "    # Begin\n",
    "    val_acc_history = []\n",
    "    data_size = {'train':dataset_size - val_size, 'val':val_size}\n",
    "    \n",
    "    print(len(dataloaders['val'].dataset))\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            print(phase)\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for item in dataloaders[phase]:\n",
    "\n",
    "                inputs = item['image']\n",
    "                labels = item['label']\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                \n",
    "                    # Get model outputs and calculate loss\n",
    "                    outputs = model(Variable(inputs))\n",
    "                    loss = criterion(outputs, Variable(labels))\n",
    "                    \n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in trainig phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            epoch_loss = running_loss / data_size[phase]\n",
    "            epoch_acc = (1.0 * running_corrects.item()) / data_size[phase]\n",
    "\n",
    "            # print validation phase statistics\n",
    "            if phase == 'val':\n",
    "                print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "                \n",
    "            torch.save(model.state_dict(), 'one_layer_models/epoch_'+str(epoch)+'.wts')\n",
    "        \n",
    "        print () \n",
    "    \n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    # test accuracy\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    for item in dataloaders['test']:\n",
    "        inputs = item['image']\n",
    "        labels = item['label']\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(Variable(inputs))\n",
    "            loss = criterion(outputs, Variable(labels))\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "    epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "    epoch_acc = (1.0 * running_corrects.item()) / len(dataloaders['test'].dataset)\n",
    "\n",
    "    print('{} loss: {:.4f} Acc: {:.4f}'.format('test', epoch_loss, epoch_acc))\n",
    "    print()\n",
    "    \n",
    "    # save the best model weights\n",
    "    torch.save(model.state_dict(), 'one_layer_best_model.wts')\n",
    "    \n",
    "    return model, val_acc_history\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2997\n",
      "Epoch 1/5\n",
      "train\n",
      "val\n",
      "val Loss: 0.4186 Acc: 0.8763\n",
      "\n",
      "Epoch 2/5\n",
      "train\n",
      "val\n",
      "val Loss: 0.2217 Acc: 0.9298\n",
      "\n",
      "Epoch 3/5\n",
      "train\n",
      "val\n",
      "val Loss: 0.1766 Acc: 0.9365\n",
      "\n",
      "Epoch 4/5\n",
      "train\n",
      "val\n",
      "val Loss: 0.1600 Acc: 0.9465\n",
      "\n",
      "Epoch 5/5\n",
      "train\n",
      "val\n",
      "val Loss: 0.1566 Acc: 0.9431\n",
      "\n",
      "Best val Acc: 0.946488\n",
      "test loss: 0.2097 Acc: 0.9206\n",
      "\n",
      "CPU times: user 1h 10min 27s, sys: 8min 4s, total: 1h 18min 32s\n",
      "Wall time: 41min 50s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(ResNet(\n",
       "   (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "   (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (relu): ReLU(inplace)\n",
       "   (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "   (layer1): Sequential(\n",
       "     (0): BasicBlock(\n",
       "       (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (relu): ReLU(inplace)\n",
       "       (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "     (1): BasicBlock(\n",
       "       (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (relu): ReLU(inplace)\n",
       "       (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (layer2): Sequential(\n",
       "     (0): BasicBlock(\n",
       "       (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "       (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (relu): ReLU(inplace)\n",
       "       (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (downsample): Sequential(\n",
       "         (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "         (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (1): BasicBlock(\n",
       "       (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (relu): ReLU(inplace)\n",
       "       (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (layer3): Sequential(\n",
       "     (0): BasicBlock(\n",
       "       (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "       (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (relu): ReLU(inplace)\n",
       "       (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (downsample): Sequential(\n",
       "         (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "         (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (1): BasicBlock(\n",
       "       (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (relu): ReLU(inplace)\n",
       "       (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (layer4): Sequential(\n",
       "     (0): BasicBlock(\n",
       "       (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "       (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (relu): ReLU(inplace)\n",
       "       (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (downsample): Sequential(\n",
       "         (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "         (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (1): BasicBlock(\n",
       "       (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (relu): ReLU(inplace)\n",
       "       (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "   (fc): Linear(in_features=512, out_features=4, bias=True)\n",
       " ),\n",
       " [0.8762541806020067,\n",
       "  0.9297658862876255,\n",
       "  0.9364548494983278,\n",
       "  0.9464882943143813,\n",
       "  0.9431438127090301])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time train(resnet18, data_loaders, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace)\n",
      "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (5): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "SSDTypeResnet(\n",
      "  (tillLayer2resnet): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (avgpool2): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc2): Linear(in_features=128, out_features=4, bias=True)\n",
      "  (afterLayer2resnet): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# two layer model v1 architecture\n",
    "class SSDTypeResnet(nn.Module):\n",
    "    def __init__(self, resnet18):\n",
    "        super(SSDTypeResnet, self).__init__()\n",
    "        rc = list(resnet18.children())\n",
    "        self.tillLayer2resnet = nn.Sequential(*rc[:6])\n",
    "        self.avgpool2 = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc2 = torch.nn.Linear(128, 4)\n",
    "        self.afterLayer2resnet = nn.Sequential(*rc[6:8])\n",
    "        self.avgpool = rc[8]\n",
    "        self.fc = rc[9]\n",
    "        print(self.tillLayer2resnet)\n",
    "        print(self.afterLayer2resnet)\n",
    "        \n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.tillLayer2resnet(x)\n",
    "        \n",
    "        # added a prediction from layer 2 as well\n",
    "        y = self.avgpool2(x)\n",
    "        y = y.view(y.size(0), -1)\n",
    "        out2 = self.fc2(y)\n",
    "        \n",
    "        x = self.afterLayer2resnet(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out1 = self.fc(x)\n",
    "\n",
    "        return out1, out2\n",
    "\n",
    "\n",
    "ssdResnet = SSDTypeResnet(resnet18) \n",
    "\n",
    "# aT = resnet18.state_dict()['conv1.weight']\n",
    "# bT = ssdResnet.state_dict()['tillLayer2resnet.0.weight']\n",
    "\n",
    "print(ssdResnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSDTypeResnet2(\n",
      "  (tillLayer3resnet): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer4first): BasicBlock(\n",
      "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool2): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc2): Linear(in_features=512, out_features=4, bias=True)\n",
      "  (layer4second): BasicBlock(\n",
      "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# two layer model v2 architecture\n",
    "class SSDTypeResnet2(nn.Module):\n",
    "    def __init__(self, resnet18):\n",
    "        super(SSDTypeResnet2, self).__init__()\n",
    "        rc = list(resnet18.children())\n",
    "        self.tillLayer3resnet = nn.Sequential(*rc[:7])\n",
    "        self.layer4first = rc[7][0]\n",
    "        self.avgpool2 = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc2 = torch.nn.Linear(512, 4)\n",
    "        self.layer4second = rc[7][1]\n",
    "        self.avgpool = rc[8]\n",
    "        self.fc = rc[9]\n",
    "        \n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.tillLayer3resnet(x)\n",
    "        x = self.layer4first(x)\n",
    "        \n",
    "        # added a prediction from the conv layer of layer 4 also\n",
    "        y = self.avgpool2(x)\n",
    "        y = y.view(y.size(0), -1)\n",
    "        out2 = self.fc2(y)\n",
    "        \n",
    "        x = self.layer4second(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out1 = self.fc(x)\n",
    "\n",
    "        return out1, out2\n",
    "\n",
    "\n",
    "ssdResnet2 = SSDTypeResnet2(resnet18) \n",
    "\n",
    "# aT = resnet18.state_dict()['conv1.weight']\n",
    "# bT = ssdResnet.state_dict()['tillLayer2resnet.0.weight']\n",
    "\n",
    "print(ssdResnet2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSDTypeResnet3(\n",
      "  (tillLayer3resnet): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer4first): BasicBlock(\n",
      "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool2): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (layer4second): BasicBlock(\n",
      "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=1024, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# two layer model v3 architecture\n",
    "class SSDTypeResnet3(nn.Module):\n",
    "    def __init__(self, resnet18):\n",
    "        super(SSDTypeResnet3, self).__init__()\n",
    "        rc = list(resnet18.children())\n",
    "        self.tillLayer3resnet = nn.Sequential(*rc[:7])\n",
    "        self.layer4first = rc[7][0]\n",
    "        self.avgpool2 = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.layer4second = rc[7][1]\n",
    "        self.avgpool = rc[8]\n",
    "        self.fc = nn.Linear(1024,4)\n",
    "        \n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.tillLayer3resnet(x)\n",
    "        x = self.layer4first(x)\n",
    "        \n",
    "        y = self.avgpool2(x)\n",
    "        y = y.view(y.size(0), -1)\n",
    "\n",
    "        x = self.layer4second(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # concatenated vectors from the last layer and from after \n",
    "        # the second conv layer in layer 4 of resnet \n",
    "        # and predicted using fc layer on that feature\n",
    "        x = torch.cat((x,y),1)\n",
    "        out = self.fc(x)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "ssdResnet3 = SSDTypeResnet3(resnet18) \n",
    "\n",
    "# aT = resnet18.state_dict()['conv1.weight']\n",
    "# bT = ssdResnet.state_dict()['tillLayer2resnet.0.weight']\n",
    "\n",
    "print(ssdResnet3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Two Layer Detection (SSD)\n",
    "def train_two_layer(model, dataloaders, criterion, optimizer, num_epochs):\n",
    "    # Begin\n",
    "    val_acc_history = []\n",
    "    data_size = {'train':dataset_size - val_size, 'val':val_size}\n",
    "    \n",
    "    print(len(dataloaders['val'].dataset))\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            print(phase)\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for item in dataloaders[phase]:\n",
    "                inputs = item['image']\n",
    "                labels = item['label']\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                \n",
    "                    # Get model outputs and calculate loss\n",
    "                    # output1, output2 = model(Variable(inputs))\n",
    "                    output1 = model(Variable(inputs))\n",
    "                    loss = criterion(output1, Variable(labels))\n",
    "                    # loss1 = criterion(output1, Variable(labels))\n",
    "                    #loss2 = criterion(output2, Variable(labels))\n",
    "                    #loss = loss1 + loss2\n",
    "                    \n",
    "                    _, preds1 = torch.max(output1, 1)\n",
    "                    #_, preds2 = torch.max(output2, 1)\n",
    "\n",
    "                    # backward + optimize only if in trainig phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "                sum1 = torch.sum(preds1 == labels.data)\n",
    "                #sum2 = torch.sum(preds2 == labels.data)\n",
    "                running_corrects += (sum1)\n",
    "                \n",
    "            epoch_loss = running_loss / data_size[phase]\n",
    "            epoch_acc = (1.0 * running_corrects.item()) / ( data_size[phase])\n",
    "\n",
    "            if phase == 'val':\n",
    "                print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "                \n",
    "            torch.save(model.state_dict(), 'two_layer_models/epoch_'+str(epoch)+'.wts')\n",
    "        \n",
    "        print () \n",
    "    \n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    # test accuracy\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    for item in dataloaders['test']:\n",
    "        inputs = item['image']\n",
    "        labels = item['label']\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            # output1, output2 = model(Variable(inputs))\n",
    "            output1 = model(Variable(inputs))\n",
    "            loss = criterion(output1, Variable(labels))\n",
    "            # loss2 = criterion(output2, Variable(labels))\n",
    "            # loss = loss1 + loss2\n",
    "            _, pred1 = torch.max(output1, 1)\n",
    "            # _, pred2 = torch.max(output2, 1)\n",
    "            \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        sum1 = torch.sum(pred1 == labels.data)\n",
    "        # sum2 = torch.sum(pred2 == labels.data)\n",
    "        running_corrects += (sum1)\n",
    "        \n",
    "    epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "    epoch_acc = (1.0 * running_corrects.item()) / (len(dataloaders['test'].dataset))\n",
    "\n",
    "    print('{} loss: {:.4f} Acc: {:.4f}'.format('test', epoch_loss, epoch_acc))\n",
    "    print()\n",
    "    \n",
    "    # save the best model weights\n",
    "    torch.save(model.state_dict(), 'two_layer_best_model.wts')\n",
    "    \n",
    "    return model, val_acc_history\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2997\n",
      "Epoch 1/5\n",
      "train\n",
      "val\n",
      "val Loss: 1.7407 Acc: 0.7140\n",
      "\n",
      "Epoch 2/5\n",
      "train\n",
      "val\n",
      "val Loss: 1.5611 Acc: 0.7207\n",
      "\n",
      "Epoch 3/5\n",
      "train\n",
      "val\n",
      "val Loss: 1.4920 Acc: 0.7291\n",
      "\n",
      "Epoch 4/5\n",
      "train\n",
      "val\n",
      "val Loss: 1.4847 Acc: 0.7291\n",
      "\n",
      "Epoch 5/5\n",
      "train\n",
      "val\n",
      "val Loss: 1.4450 Acc: 0.7341\n",
      "\n",
      "Best val Acc: 0.734114\n",
      "test loss: 1.4340 Acc: 0.7424\n",
      "\n",
      "CPU times: user 1h 11min 47s, sys: 8min 44s, total: 1h 20min 32s\n",
      "Wall time: 43min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(SSDTypeResnet(\n",
       "   (tillLayer2resnet): Sequential(\n",
       "     (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "     (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): ReLU(inplace)\n",
       "     (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "     (4): Sequential(\n",
       "       (0): BasicBlock(\n",
       "         (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "         (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         (relu): ReLU(inplace)\n",
       "         (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "         (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (1): BasicBlock(\n",
       "         (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "         (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         (relu): ReLU(inplace)\n",
       "         (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "         (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (5): Sequential(\n",
       "       (0): BasicBlock(\n",
       "         (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "         (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         (relu): ReLU(inplace)\n",
       "         (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "         (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         (downsample): Sequential(\n",
       "           (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "           (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         )\n",
       "       )\n",
       "       (1): BasicBlock(\n",
       "         (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "         (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         (relu): ReLU(inplace)\n",
       "         (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "         (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (avgpool2): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "   (fc2): Linear(in_features=128, out_features=4, bias=True)\n",
       "   (afterLayer2resnet): Sequential(\n",
       "     (0): Sequential(\n",
       "       (0): BasicBlock(\n",
       "         (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "         (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         (relu): ReLU(inplace)\n",
       "         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "         (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         (downsample): Sequential(\n",
       "           (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "           (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         )\n",
       "       )\n",
       "       (1): BasicBlock(\n",
       "         (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "         (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         (relu): ReLU(inplace)\n",
       "         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "         (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (1): Sequential(\n",
       "       (0): BasicBlock(\n",
       "         (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "         (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         (relu): ReLU(inplace)\n",
       "         (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "         (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         (downsample): Sequential(\n",
       "           (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "           (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         )\n",
       "       )\n",
       "       (1): BasicBlock(\n",
       "         (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "         (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         (relu): ReLU(inplace)\n",
       "         (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "         (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "   (fc): Linear(in_features=512, out_features=4, bias=True)\n",
       " ),\n",
       " [0.7140468227424749,\n",
       "  0.7207357859531772,\n",
       "  0.7290969899665551,\n",
       "  0.7290969899665551,\n",
       "  0.7341137123745819])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time train_two_layer(ssdResnet, data_loaders, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2997\n",
      "Epoch 1/5\n",
      "train\n",
      "val\n",
      "val Loss: 1.5269 Acc: 0.6137\n",
      "\n",
      "Epoch 2/5\n",
      "train\n",
      "val\n",
      "val Loss: 1.5080 Acc: 0.6622\n",
      "\n",
      "Epoch 3/5\n",
      "train\n",
      "val\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-b384425b9a39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time train_two_layer(ssdResnet2, data_loaders, criterion, optimizer, num_epochs)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/amrits/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2158\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amrits/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2079\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2081\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2082\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/home/amrits/anaconda2/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amrits/anaconda2/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-4e2705400527>\u001b[0m in \u001b[0;36mtrain_two_layer\u001b[0;34m(model, dataloaders, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                     \u001b[0;31m# Get model outputs and calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                     \u001b[0moutput1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0mloss1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amrits/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-f5e3960a318a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtillLayer3resnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4first\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amrits/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amrits/anaconda2/lib/python2.7/site-packages/torch/nn/modules/container.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amrits/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amrits/anaconda2/lib/python2.7/site-packages/torch/nn/modules/container.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amrits/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amrits/anaconda2/lib/python2.7/site-packages/torchvision/models/resnet.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amrits/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amrits/anaconda2/lib/python2.7/site-packages/torch/nn/modules/conv.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%time train_two_layer(ssdResnet2, data_loaders, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2997\n",
      "Epoch 1/5\n",
      "train\n",
      "val\n",
      "val Loss: 0.8354 Acc: 0.7860\n",
      "\n",
      "Epoch 2/5\n",
      "train\n",
      "val\n",
      "val Loss: 0.5551 Acc: 0.8863\n",
      "\n",
      "Epoch 3/5\n",
      "train\n",
      "val\n",
      "val Loss: 0.4512 Acc: 0.9097\n",
      "\n",
      "Epoch 4/5\n",
      "train\n",
      "val\n",
      "val Loss: 0.3910 Acc: 0.9064\n",
      "\n",
      "Epoch 5/5\n",
      "train\n",
      "val\n",
      "val Loss: 0.3482 Acc: 0.9197\n",
      "\n",
      "Best val Acc: 0.919732\n",
      "test loss: 0.3581 Acc: 0.9229\n",
      "\n",
      "CPU times: user 1h 11min 20s, sys: 8min 6s, total: 1h 19min 27s\n",
      "Wall time: 40min 56s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(SSDTypeResnet3(\n",
       "   (tillLayer3resnet): Sequential(\n",
       "     (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "     (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): ReLU(inplace)\n",
       "     (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "     (4): Sequential(\n",
       "       (0): BasicBlock(\n",
       "         (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "         (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         (relu): ReLU(inplace)\n",
       "         (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "         (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (1): BasicBlock(\n",
       "         (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "         (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         (relu): ReLU(inplace)\n",
       "         (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "         (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (5): Sequential(\n",
       "       (0): BasicBlock(\n",
       "         (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "         (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         (relu): ReLU(inplace)\n",
       "         (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "         (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         (downsample): Sequential(\n",
       "           (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "           (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         )\n",
       "       )\n",
       "       (1): BasicBlock(\n",
       "         (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "         (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         (relu): ReLU(inplace)\n",
       "         (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "         (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (6): Sequential(\n",
       "       (0): BasicBlock(\n",
       "         (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "         (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         (relu): ReLU(inplace)\n",
       "         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "         (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         (downsample): Sequential(\n",
       "           (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "           (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         )\n",
       "       )\n",
       "       (1): BasicBlock(\n",
       "         (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "         (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         (relu): ReLU(inplace)\n",
       "         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "         (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (layer4first): BasicBlock(\n",
       "     (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (avgpool2): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "   (layer4second): BasicBlock(\n",
       "     (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "   (fc): Linear(in_features=1024, out_features=4, bias=True)\n",
       " ),\n",
       " [0.7859531772575251,\n",
       "  0.8862876254180602,\n",
       "  0.9096989966555183,\n",
       "  0.9063545150501672,\n",
       "  0.919732441471572])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# two layer v3\n",
    "%time train_two_layer(ssdResnet3, data_loaders, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Accuracy Calculation\n",
    "For applying detection, use a slding window method to test the above trained trained network on the detection task:<br/>\n",
    "Take some windows of varying size and aspect ratios and slide it through the test image (considering some stride of pixels) from left to right, and top to bottom, detect the class scores for each of the window, and keep only those which are above a certain threshold value. There is a similar approach used in the paper -Faster RCNN by Ross Girshick, where he uses three diferent scales/sizes and three different aspect ratios, making a total of nine windows per pixel to slide. You need to write the code and use it in testing code to find the predicted boxes and their classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to run through sliding windows over the image\n",
    "def sliding_window(img, model, predict_class, sthreshold=0.98):\n",
    "    selec_threshold = sthreshold\n",
    "    boxes_a = []\n",
    "    boxes_b = []\n",
    "    boxes_c = []\n",
    "    \n",
    "    # the various window sizes and aspect ratios to be chosen\n",
    "    window_sizes = [128, 224, 480]\n",
    "    aspect_ratios = [1, 0.5, 2]\n",
    "    strides = [32]\n",
    "    for window_size in window_sizes:\n",
    "        for aspect_ratio in aspect_ratios:\n",
    "            for stride in strides:\n",
    "                img_batch = []\n",
    "                bndbox_batch = []\n",
    "                for y in range(0, img.size[1], stride):\n",
    "                    for x in range(0, img.size[0], stride):\n",
    "                        x1 = x\n",
    "                        y1 = y\n",
    "                        x2 = x1 + window_size*aspect_ratio\n",
    "                        y2 = y1 + window_size\n",
    "                        # check if it extends outside image borders\n",
    "                        if x2 > img.size[0]:\n",
    "                            break\n",
    "                           \n",
    "                        # crop the image, and append it to the batch\n",
    "                        cropped_img = img.crop((x1,y1,x2,y2))\n",
    "                        rescaled_img = composed_transform(cropped_img)\n",
    "                        img_batch.append(rescaled_img)\n",
    "                        bndbox_batch.append([x1, y1, x2, y2])\n",
    "                        \n",
    "                    # check if it extends outside image borders\n",
    "                    if y2 > img.size[1]:\n",
    "                        break\n",
    "                \n",
    "                if len(img_batch) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # predict the class labels of the img windows\n",
    "                prediction_batch = predict_class(torch.stack(img_batch))\n",
    "                # segregate the separate bounding boxes of the different classes\n",
    "                for pred, bnd in zip(prediction_batch, bndbox_batch):\n",
    "                    if pred[0] == classes.index('__background__'):\n",
    "                        continue\n",
    "                    elif pred[0] == classes.index('aeroplane') and pred[1] > selec_threshold: \n",
    "                        boxes_a.append([pred[1]] + bnd)\n",
    "                    elif pred[0] == classes.index('bottle') and pred[1] > selec_threshold: \n",
    "                        boxes_b.append([pred[1]] + bnd)\n",
    "                    elif pred[0] == classes.index('chair') and pred[1] > selec_threshold:\n",
    "                        boxes_c.append([pred[1]] + bnd)\n",
    "                        \n",
    "    return boxes_a, boxes_b, boxes_c\n",
    "\n",
    "                        \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply non_maximum_supression to reduce the number of boxes. You are free to choose the threshold value for non maximum supression, but choose wisely [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non Maximum Suppression\n",
    "def non_maximum_supression(boxes,image,threshold = 0.01):\n",
    "    keep_boxes = []\n",
    "    # if the bBoxes list is empty, then return empty list\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    boxes = np.array(boxes)\n",
    "    # the get list of separate coordinates \n",
    "    x1 = boxes[:, 1]\n",
    "    y1 = boxes[:, 2]\n",
    "    x2 = boxes[:, 3]\n",
    "    y2 = boxes[:, 4]\n",
    "    # get the probability scores of all boxes\n",
    "    scores = boxes[:, 0]\n",
    "    # get the areas of all boxes\n",
    "    areas = (x2 - x1 + 1)*(y2 - y1 + 1)\n",
    "    \n",
    "    # sort in decreasing order of probability scores\n",
    "    order = scores.argsort()[::-1]\n",
    "    \n",
    "    # decide which all boxes to keep\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        # take the highest probabilyt box remaining\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "\n",
    "        # discard all boxes that match this box considerably\n",
    "        inds = np.where(ovr <= threshold)[0]\n",
    "        order = order[inds + 1]\n",
    "    for args in keep:\n",
    "        keep_boxes.append(boxes[args].tolist() + [image])\n",
    "        \n",
    "\n",
    "    return keep_boxes\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the trained model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One layer detection\n",
    "def test(model):\n",
    "    \n",
    "    # load best model weights\n",
    "    model_wts = torch.load('one_layer_v2/one_layer_best_model.wts')\n",
    "    \n",
    "    model.load_state_dict(model_wts)\n",
    "    model.eval()\n",
    "    print(\"Model Loaded\")\n",
    "    \n",
    "    pred_bndboxes = [[],[],[],[]]\n",
    "    actual = {}\n",
    "    pr = [[],[],[],[]]  \n",
    "    re = [[],[],[],[]]  \n",
    "    \n",
    "    # function to predict the class of given images, using our trained model\n",
    "    def predict_class(inputs):\n",
    "        # get the raw output from the model\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(Variable(inputs))\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        preds = preds.tolist()\n",
    "        # convert to probability scores\n",
    "        prob_layer = nn.Softmax(dim=1)\n",
    "        probs = prob_layer(outputs).tolist()\n",
    "        out_probs = np.amax(probs, axis=1)\n",
    "        # return bbox along with the prob score\n",
    "        return list(zip(preds, out_probs))\n",
    "        \n",
    "        \n",
    "    # print bounding box on the image    \n",
    "    def print_boxes(img, bbs, color):\n",
    "        for bb in bbs:\n",
    "            cv2.rectangle(img, (int(bb[1]), int(bb[2])), (int(bb[3]), int(bb[4])), color, 2)\n",
    "        return img\n",
    "        \n",
    "    count = 0\n",
    "    allowed_labels = ['aeroplane', 'bottle', 'chair']\n",
    "    label_counts = [0,0,0,0]\n",
    "    test_images_folder = os.path.join('./test', 'VOC2007/JPEGImages/')\n",
    "    test_objects_folder = os.path.join('./test', 'VOC2007/Annotations/')\n",
    "    # read the test images\n",
    "    for idx, image_name in enumerate(os.listdir(test_images_folder)):\n",
    "        image_file = os.path.join(test_images_folder, image_name)\n",
    "        objects_file = os.path.join(test_objects_folder, image_name.split(\".\")[0]+'.xml')\n",
    "        tree = ET.parse(objects_file)\n",
    "        img = Image.open(image_file).convert('RGB')\n",
    "        root = tree.getroot()\n",
    "        labels = []\n",
    "        bndboxes = []\n",
    "        for obj in root.findall('object'):\n",
    "            label = obj.find('name').text\n",
    "            if label not in allowed_labels:\n",
    "                continue\n",
    "            label_counts[classes.index(label)] += 1\n",
    "            bndbox = obj.find('bndbox')\n",
    "            coord = []\n",
    "            for cd in bndbox:\n",
    "                coord.append(int(cd.text))\n",
    "            labels.append(label)\n",
    "            bndboxes.append(coord)\n",
    "            \n",
    "        # ignore the image if no object present in it\n",
    "        if len(labels) == 0:\n",
    "            continue\n",
    "        count = count + 1\n",
    "        print(idx+1, \": \", count, \": \", image_name)\n",
    "    \n",
    "        # run the sliding window approach to extract all possible bounding boxes with objects\n",
    "        boxes_a, boxes_b, boxes_c = sliding_window(img, model, predict_class)\n",
    "        \n",
    "        # run NMS separately on all boxes of a specific class to get the reduced number of boxes\n",
    "        pred_boxes = [[],[],[],[]]\n",
    "        pred_boxes[classes.index('aeroplane')] = non_maximum_supression(boxes_a, image_name)\n",
    "        pred_boxes[classes.index('bottle')] = non_maximum_supression(boxes_b, image_name)\n",
    "        pred_boxes[classes.index('chair')] = non_maximum_supression(boxes_c, image_name)\n",
    "             \n",
    "        # save the predicted object Bboxes in the image to the global list\n",
    "        for idx, class_name in enumerate(classes):\n",
    "            if class_name == '__background__':\n",
    "                continue\n",
    "            pred_bndboxes[idx] = pred_bndboxes[idx] + pred_boxes[idx]\n",
    "            \n",
    "        # print the bBoxes on the image and store it\n",
    "        img = cv2.imread(image_file)\n",
    "        img = print_boxes(img, pred_boxes[classes.index('aeroplane')], (255,0,0)) #blue \n",
    "        img = print_boxes(img, pred_boxes[classes.index('bottle')], (0,255,0))    #green\n",
    "        img = print_boxes(img, pred_boxes[classes.index('chair')], (0,0,255))     #red\n",
    "        cv2.imwrite(\"outputs/\"+image_name, img)\n",
    "        actual[image_name] = [[],[],[],[]]\n",
    "        \n",
    "        # save the actual object bBoxes in the image to the global\n",
    "        for lbl, bd in zip(labels, bndboxes):\n",
    "            actual[image_name][classes.index(lbl)].append(bd)\n",
    "    \n",
    "    # calculate the precision and recall vectors for each class\n",
    "    correct = [[],[],[],[]]\n",
    "    for idx, class_name in enumerate(classes):\n",
    "        if class_name == '__background__':\n",
    "            continue\n",
    "        pr[idx], re[idx], correct[idx] = calculate_precision_recall(pred_bndboxes[idx], actual, idx, label_counts[idx])\n",
    "    \n",
    "    # get the Average precision for each class\n",
    "    ap = [0, 0, 0, 0]\n",
    "    for idx, class_name in enumerate(classes):\n",
    "        if class_name == '__background__':\n",
    "            continue\n",
    "        # print(pr[idx],re[idx])\n",
    "        ap[idx] = calculate_average_precision(pr[idx], re[idx])\n",
    "        \n",
    "    # print the average precision for all classes\n",
    "    # and the correctly predicted images for all classes\n",
    "    ap_sum = 0.0\n",
    "    for idx, class_name in enumerate(classes):\n",
    "        if class_name == '__background__':\n",
    "            continue\n",
    "        print(\"Average Precision for class \"+class_name+\" is \"+str(ap[idx]))\n",
    "        print(\"Correctly predicted images for class \"+class_name+\" are \"+str(correct[idx]))\n",
    "        ap_sum += ap[idx]\n",
    "     \n",
    "    # get the mAP score as the average of AP of all classes\n",
    "    mAP = ap_sum/(len(ap)-1)\n",
    "    \n",
    "    print(\"MAP:\",mAP)\n",
    "    return mAP\n",
    "        \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate AP over a class\n",
    "def calculate_average_precision(pr, re):\n",
    "    sre, spr = zip(*sorted(zip(re, pr)))\n",
    "    ipr = []       # to store the interpolated precision\n",
    "    \n",
    "    # to get the interpolated precision from the sorted precision values\n",
    "    for i, p in enumerate(reversed(spr)):\n",
    "        if i == 0:\n",
    "            ipr.append(p)\n",
    "        elif p <= ipr[i-1]:\n",
    "            ipr.append(ipr[i-1])\n",
    "        else:\n",
    "            ipr.append(p)\n",
    "        \n",
    "    ipr.reverse()\n",
    "    \n",
    "    # the values of recall at which the ipr is taken\n",
    "    markers= [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "    mark = 0\n",
    "    ipr_sum = 0.0\n",
    "    count = 0\n",
    "    \n",
    "    # to get sum of ipr at all these recall values\n",
    "    for i,r in enumerate(sre):\n",
    "        if r >= markers[mark]:\n",
    "            ipr_sum += ipr[i]\n",
    "            while(mark <= 10 and markers[mark] <= r):\n",
    "                mark += 1\n",
    "            count += 1\n",
    "        if mark > 10:\n",
    "            break\n",
    "    \n",
    "    ap = ipr_sum/count\n",
    "    return ap\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the precision and recall at each instance of the class\n",
    "def calculate_precision_recall(pred_bndboxes, actual, label_idx, label_count):\n",
    "    \n",
    "    tp = 0\n",
    "    # sort the predictions in decreasing order of probability scores\n",
    "    pred_sorted = sorted(pred_bndboxes, key=lambda x: x[0])\n",
    "    precision = []\n",
    "    recall = []\n",
    "    correct = []\n",
    "    \n",
    "    # iterating over the predictions \n",
    "    for idx, prb in enumerate(pred_sorted):\n",
    "        flag = 0\n",
    "        img = prb[5]\n",
    "        # check the IoU match of the predicition with the\n",
    "        # objects in that image\n",
    "        for acb in actual[img][label_idx]:\n",
    "            iou = get_iou(acb, prb[1:5])\n",
    "            \n",
    "            # count as a True Positive (tp) only if IoU >= 0.5\n",
    "            if iou >= 0.5:\n",
    "                flag = 1\n",
    "                correct.append(img)\n",
    "                break\n",
    "        tp = tp + flag \n",
    "    \n",
    "        # get precision and recall at this rank in the list\n",
    "        precision.append(1.0*tp/(idx+1))\n",
    "        recall.append(1.0*tp/label_count)\n",
    "        \n",
    "    # return the complete precision and recall vectors\n",
    "    # return the correctly predicted images\n",
    "    return precision, recall, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n",
      "9 :  1 :  009570.jpg\n",
      "10 :  2 :  004055.jpg\n",
      "16 :  3 :  004964.jpg\n",
      "23 :  4 :  005537.jpg\n",
      "29 :  5 :  000573.jpg\n",
      "36 :  6 :  005622.jpg\n",
      "53 :  7 :  009742.jpg\n",
      "54 :  8 :  001921.jpg\n",
      "56 :  9 :  005725.jpg\n",
      "72 :  10 :  007348.jpg\n",
      "74 :  11 :  005043.jpg\n",
      "77 :  12 :  002907.jpg\n",
      "81 :  13 :  000316.jpg\n",
      "86 :  14 :  007738.jpg\n",
      "89 :  15 :  005907.jpg\n",
      "92 :  16 :  006263.jpg\n",
      "93 :  17 :  009012.jpg\n",
      "99 :  18 :  009663.jpg\n",
      "102 :  19 :  008131.jpg\n",
      "103 :  20 :  004262.jpg\n",
      "117 :  21 :  005827.jpg\n",
      "119 :  22 :  003544.jpg\n",
      "121 :  23 :  004744.jpg\n",
      "126 :  24 :  005286.jpg\n",
      "127 :  25 :  005218.jpg\n",
      "128 :  26 :  007564.jpg\n",
      "137 :  27 :  000277.jpg\n",
      "142 :  28 :  000642.jpg\n",
      "151 :  29 :  005103.jpg\n",
      "152 :  30 :  003649.jpg\n",
      "157 :  31 :  008646.jpg\n",
      "159 :  32 :  005050.jpg\n",
      "163 :  33 :  002583.jpg\n",
      "165 :  34 :  005474.jpg\n",
      "175 :  35 :  009846.jpg\n",
      "179 :  36 :  008110.jpg\n",
      "195 :  37 :  009076.jpg\n",
      "206 :  38 :  005994.jpg\n",
      "211 :  39 :  003532.jpg\n",
      "218 :  40 :  000696.jpg\n",
      "225 :  41 :  007698.jpg\n",
      "227 :  42 :  006780.jpg\n",
      "229 :  43 :  009311.jpg\n",
      "235 :  44 :  005040.jpg\n",
      "241 :  45 :  004377.jpg\n",
      "250 :  46 :  005196.jpg\n",
      "252 :  47 :  004923.jpg\n",
      "257 :  48 :  002007.jpg\n",
      "258 :  49 :  000124.jpg\n",
      "263 :  50 :  008986.jpg\n",
      "268 :  51 :  003761.jpg\n",
      "269 :  52 :  007973.jpg\n",
      "271 :  53 :  001433.jpg\n",
      "286 :  54 :  008991.jpg\n",
      "292 :  55 :  003842.jpg\n",
      "293 :  56 :  001305.jpg\n",
      "299 :  57 :  008113.jpg\n",
      "307 :  58 :  009171.jpg\n",
      "308 :  59 :  001712.jpg\n",
      "310 :  60 :  001660.jpg\n",
      "324 :  61 :  003736.jpg\n",
      "325 :  62 :  008627.jpg\n",
      "336 :  63 :  005076.jpg\n",
      "345 :  64 :  009962.jpg\n",
      "349 :  65 :  001829.jpg\n",
      "350 :  66 :  006297.jpg\n",
      "354 :  67 :  007722.jpg\n",
      "366 :  68 :  000397.jpg\n",
      "371 :  69 :  002357.jpg\n",
      "373 :  70 :  006293.jpg\n",
      "376 :  71 :  005008.jpg\n",
      "381 :  72 :  002705.jpg\n",
      "394 :  73 :  008245.jpg\n",
      "397 :  74 :  009835.jpg\n",
      "405 :  75 :  005734.jpg\n",
      "410 :  76 :  000090.jpg\n",
      "411 :  77 :  005721.jpg\n",
      "414 :  78 :  006360.jpg\n",
      "416 :  79 :  004348.jpg\n",
      "419 :  80 :  004227.jpg\n",
      "420 :  81 :  008039.jpg\n",
      "423 :  82 :  008431.jpg\n",
      "429 :  83 :  004820.jpg\n",
      "433 :  84 :  005927.jpg\n",
      "438 :  85 :  008778.jpg\n",
      "444 :  86 :  003770.jpg\n",
      "445 :  87 :  004780.jpg\n",
      "460 :  88 :  008950.jpg\n",
      "461 :  89 :  002644.jpg\n",
      "463 :  90 :  002813.jpg\n",
      "464 :  91 :  001519.jpg\n",
      "466 :  92 :  009646.jpg\n",
      "477 :  93 :  008761.jpg\n",
      "480 :  94 :  001569.jpg\n",
      "489 :  95 :  002100.jpg\n",
      "490 :  96 :  008821.jpg\n",
      "492 :  97 :  003341.jpg\n",
      "498 :  98 :  005002.jpg\n",
      "499 :  99 :  003755.jpg\n",
      "505 :  100 :  007548.jpg\n",
      "506 :  101 :  007747.jpg\n",
      "517 :  102 :  003942.jpg\n",
      "524 :  103 :  002908.jpg\n",
      "530 :  104 :  009486.jpg\n",
      "535 :  105 :  001696.jpg\n",
      "537 :  106 :  001925.jpg\n",
      "543 :  107 :  002167.jpg\n",
      "546 :  108 :  008496.jpg\n",
      "550 :  109 :  005745.jpg\n",
      "563 :  110 :  008714.jpg\n",
      "564 :  111 :  002499.jpg\n",
      "566 :  112 :  007178.jpg\n",
      "568 :  113 :  006826.jpg\n",
      "569 :  114 :  007684.jpg\n",
      "571 :  115 :  009677.jpg\n",
      "578 :  116 :  000560.jpg\n",
      "580 :  117 :  002510.jpg\n",
      "588 :  118 :  008400.jpg\n",
      "589 :  119 :  007403.jpg\n",
      "590 :  120 :  001319.jpg\n",
      "605 :  121 :  008947.jpg\n",
      "608 :  122 :  002217.jpg\n",
      "609 :  123 :  005249.jpg\n",
      "620 :  124 :  000587.jpg\n",
      "631 :  125 :  008192.jpg\n",
      "632 :  126 :  000745.jpg\n",
      "646 :  127 :  004098.jpg\n",
      "650 :  128 :  005942.jpg\n",
      "656 :  129 :  008379.jpg\n",
      "659 :  130 :  001856.jpg\n",
      "661 :  131 :  009332.jpg\n",
      "663 :  132 :  009366.jpg\n",
      "664 :  133 :  000994.jpg\n",
      "672 :  134 :  000621.jpg\n",
      "673 :  135 :  002246.jpg\n",
      "677 :  136 :  007228.jpg\n",
      "681 :  137 :  002231.jpg\n",
      "683 :  138 :  004469.jpg\n",
      "703 :  139 :  003268.jpg\n",
      "707 :  140 :  001735.jpg\n",
      "719 :  141 :  008490.jpg\n",
      "720 :  142 :  001547.jpg\n",
      "739 :  143 :  006514.jpg\n",
      "743 :  144 :  007135.jpg\n",
      "758 :  145 :  008825.jpg\n",
      "764 :  146 :  004717.jpg\n",
      "766 :  147 :  001657.jpg\n",
      "767 :  148 :  007896.jpg\n",
      "769 :  149 :  001039.jpg\n",
      "777 :  150 :  002264.jpg\n",
      "781 :  151 :  000144.jpg\n",
      "782 :  152 :  005965.jpg\n",
      "790 :  153 :  001087.jpg\n",
      "806 :  154 :  008210.jpg\n",
      "811 :  155 :  003245.jpg\n",
      "814 :  156 :  009084.jpg\n",
      "820 :  157 :  008922.jpg\n",
      "827 :  158 :  004506.jpg\n",
      "832 :  159 :  007632.jpg\n",
      "837 :  160 :  007354.jpg\n",
      "838 :  161 :  008881.jpg\n",
      "840 :  162 :  003502.jpg\n",
      "842 :  163 :  002885.jpg\n",
      "845 :  164 :  004677.jpg\n",
      "847 :  165 :  007993.jpg\n",
      "849 :  166 :  001826.jpg\n",
      "851 :  167 :  007290.jpg\n",
      "868 :  168 :  006651.jpg\n",
      "871 :  169 :  002851.jpg\n",
      "880 :  170 :  002298.jpg\n",
      "890 :  171 :  003230.jpg\n",
      "891 :  172 :  006294.jpg\n",
      "896 :  173 :  003590.jpg\n",
      "897 :  174 :  004971.jpg\n",
      "901 :  175 :  004919.jpg\n",
      "906 :  176 :  004053.jpg\n",
      "910 :  177 :  003478.jpg\n",
      "911 :  178 :  004486.jpg\n",
      "913 :  179 :  008669.jpg\n",
      "919 :  180 :  007203.jpg\n",
      "920 :  181 :  002857.jpg\n",
      "922 :  182 :  000910.jpg\n",
      "930 :  183 :  004139.jpg\n",
      "951 :  184 :  002971.jpg\n",
      "954 :  185 :  009529.jpg\n",
      "967 :  186 :  001407.jpg\n",
      "975 :  187 :  006743.jpg\n",
      "976 :  188 :  006432.jpg\n",
      "977 :  189 :  004922.jpg\n",
      "985 :  190 :  005570.jpg\n",
      "994 :  191 :  001153.jpg\n",
      "995 :  192 :  003520.jpg\n",
      "1002 :  193 :  002753.jpg\n",
      "1010 :  194 :  008056.jpg\n",
      "1014 :  195 :  004127.jpg\n",
      "1016 :  196 :  005556.jpg\n",
      "1019 :  197 :  009916.jpg\n",
      "1024 :  198 :  008516.jpg\n",
      "1029 :  199 :  002365.jpg\n",
      "1036 :  200 :  002792.jpg\n",
      "1038 :  201 :  001081.jpg\n",
      "1041 :  202 :  003617.jpg\n",
      "1042 :  203 :  005044.jpg\n",
      "1046 :  204 :  008925.jpg\n",
      "1048 :  205 :  000111.jpg\n",
      "1049 :  206 :  000385.jpg\n",
      "1052 :  207 :  007752.jpg\n",
      "1055 :  208 :  004716.jpg\n",
      "1058 :  209 :  005233.jpg\n",
      "1062 :  210 :  000521.jpg\n",
      "1063 :  211 :  008537.jpg\n",
      "1066 :  212 :  005291.jpg\n",
      "1076 :  213 :  004525.jpg\n",
      "1078 :  214 :  006359.jpg\n",
      "1081 :  215 :  008678.jpg\n",
      "1088 :  216 :  002724.jpg\n",
      "1092 :  217 :  009891.jpg\n",
      "1098 :  218 :  002674.jpg\n",
      "1103 :  219 :  001742.jpg\n",
      "1108 :  220 :  001949.jpg\n",
      "1110 :  221 :  003665.jpg\n",
      "1112 :  222 :  000473.jpg\n",
      "1115 :  223 :  008405.jpg\n",
      "1116 :  224 :  002449.jpg\n",
      "1118 :  225 :  008367.jpg\n",
      "1126 :  226 :  002904.jpg\n",
      "1127 :  227 :  006453.jpg\n",
      "1132 :  228 :  007262.jpg\n",
      "1143 :  229 :  002850.jpg\n",
      "1144 :  230 :  004781.jpg\n",
      "1158 :  231 :  009782.jpg\n",
      "1159 :  232 :  007828.jpg\n",
      "1165 :  233 :  006758.jpg\n",
      "1167 :  234 :  006633.jpg\n",
      "1196 :  235 :  002157.jpg\n",
      "1202 :  236 :  001942.jpg\n",
      "1207 :  237 :  007268.jpg\n",
      "1208 :  238 :  002198.jpg\n",
      "1213 :  239 :  008089.jpg\n",
      "1214 :  240 :  000136.jpg\n",
      "1217 :  241 :  007207.jpg\n",
      "1225 :  242 :  002110.jpg\n",
      "1233 :  243 :  001222.jpg\n",
      "1236 :  244 :  009056.jpg\n",
      "1238 :  245 :  003278.jpg\n",
      "1244 :  246 :  001715.jpg\n",
      "1247 :  247 :  008861.jpg\n",
      "1269 :  248 :  002743.jpg\n",
      "1276 :  249 :  008894.jpg\n",
      "1284 :  250 :  007057.jpg\n",
      "1287 :  251 :  002503.jpg\n",
      "1295 :  252 :  000234.jpg\n",
      "1297 :  253 :  002930.jpg\n",
      "1301 :  254 :  002536.jpg\n",
      "1304 :  255 :  004695.jpg\n",
      "1305 :  256 :  000846.jpg\n",
      "1317 :  257 :  000181.jpg\n",
      "1319 :  258 :  004319.jpg\n",
      "1320 :  259 :  007676.jpg\n",
      "1325 :  260 :  001961.jpg\n",
      "1334 :  261 :  003372.jpg\n",
      "1341 :  262 :  003802.jpg\n",
      "1342 :  263 :  007500.jpg\n",
      "1346 :  264 :  006248.jpg\n",
      "1350 :  265 :  007652.jpg\n",
      "1355 :  266 :  009075.jpg\n",
      "1360 :  267 :  009211.jpg\n",
      "1361 :  268 :  002638.jpg\n",
      "1362 :  269 :  004824.jpg\n",
      "1365 :  270 :  003071.jpg\n",
      "1369 :  271 :  007267.jpg\n",
      "1372 :  272 :  002463.jpg\n",
      "1373 :  273 :  007700.jpg\n",
      "1378 :  274 :  001055.jpg\n",
      "1386 :  275 :  009397.jpg\n",
      "1390 :  276 :  003775.jpg\n",
      "1394 :  277 :  005302.jpg\n",
      "1399 :  278 :  008474.jpg\n",
      "1400 :  279 :  009482.jpg\n",
      "1402 :  280 :  007341.jpg\n",
      "1428 :  281 :  001885.jpg\n",
      "1431 :  282 :  001456.jpg\n",
      "1442 :  283 :  002161.jpg\n",
      "1448 :  284 :  001169.jpg\n",
      "1450 :  285 :  001431.jpg\n",
      "1456 :  286 :  000314.jpg\n",
      "1457 :  287 :  009009.jpg\n",
      "1459 :  288 :  003209.jpg\n",
      "1466 :  289 :  003459.jpg\n",
      "1468 :  290 :  003384.jpg\n",
      "1476 :  291 :  009355.jpg\n",
      "1481 :  292 :  009513.jpg\n",
      "1482 :  293 :  008352.jpg\n",
      "1484 :  294 :  003894.jpg\n",
      "1486 :  295 :  002467.jpg\n",
      "1491 :  296 :  009708.jpg\n",
      "1492 :  297 :  002235.jpg\n",
      "1495 :  298 :  000126.jpg\n",
      "1500 :  299 :  004640.jpg\n",
      "1501 :  300 :  008104.jpg\n",
      "1505 :  301 :  006287.jpg\n",
      "1506 :  302 :  000665.jpg\n",
      "1508 :  303 :  003431.jpg\n",
      "1511 :  304 :  004072.jpg\n",
      "1513 :  305 :  007169.jpg\n",
      "1515 :  306 :  008625.jpg\n",
      "1520 :  307 :  003123.jpg\n",
      "1523 :  308 :  001047.jpg\n",
      "1550 :  309 :  000856.jpg\n",
      "1556 :  310 :  005193.jpg\n",
      "1559 :  311 :  007569.jpg\n",
      "1561 :  312 :  007597.jpg\n",
      "1564 :  313 :  009263.jpg\n",
      "1574 :  314 :  000085.jpg\n",
      "1575 :  315 :  004733.jpg\n",
      "1585 :  316 :  009553.jpg\n",
      "1599 :  317 :  001505.jpg\n",
      "1600 :  318 :  000369.jpg\n",
      "1603 :  319 :  000414.jpg\n",
      "1605 :  320 :  001099.jpg\n",
      "1606 :  321 :  000418.jpg\n",
      "1617 :  322 :  009631.jpg\n",
      "1619 :  323 :  006406.jpg\n",
      "1627 :  324 :  006662.jpg\n",
      "1632 :  325 :  004813.jpg\n",
      "1655 :  326 :  000706.jpg\n",
      "1669 :  327 :  002654.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1678 :  328 :  006546.jpg\n",
      "1692 :  329 :  008440.jpg\n",
      "1693 :  330 :  004045.jpg\n",
      "1698 :  331 :  009356.jpg\n",
      "1706 :  332 :  007806.jpg\n",
      "1709 :  333 :  000216.jpg\n",
      "1722 :  334 :  003144.jpg\n",
      "1729 :  335 :  001086.jpg\n",
      "1730 :  336 :  005294.jpg\n",
      "1734 :  337 :  002777.jpg\n",
      "1737 :  338 :  007335.jpg\n",
      "1744 :  339 :  006830.jpg\n",
      "1745 :  340 :  009929.jpg\n",
      "1748 :  341 :  006142.jpg\n",
      "1756 :  342 :  003488.jpg\n",
      "1765 :  343 :  005323.jpg\n",
      "1778 :  344 :  006051.jpg\n",
      "1779 :  345 :  000008.jpg\n",
      "1780 :  346 :  003574.jpg\n",
      "1788 :  347 :  008198.jpg\n",
      "1789 :  348 :  008563.jpg\n",
      "1791 :  349 :  003221.jpg\n",
      "1796 :  350 :  001814.jpg\n",
      "1799 :  351 :  009154.jpg\n",
      "1802 :  352 :  004032.jpg\n",
      "1804 :  353 :  006923.jpg\n",
      "1811 :  354 :  009222.jpg\n",
      "1833 :  355 :  009802.jpg\n",
      "1840 :  356 :  001783.jpg\n",
      "1843 :  357 :  008153.jpg\n",
      "1847 :  358 :  005226.jpg\n",
      "1853 :  359 :  005313.jpg\n",
      "1856 :  360 :  002301.jpg\n",
      "1857 :  361 :  006592.jpg\n",
      "1868 :  362 :  000817.jpg\n",
      "1872 :  363 :  005216.jpg\n",
      "1877 :  364 :  004645.jpg\n",
      "1879 :  365 :  007202.jpg\n",
      "1894 :  366 :  000762.jpg\n",
      "1895 :  367 :  004752.jpg\n",
      "1902 :  368 :  003251.jpg\n",
      "1904 :  369 :  003600.jpg\n",
      "1912 :  370 :  000606.jpg\n",
      "1916 :  371 :  000151.jpg\n",
      "1919 :  372 :  000510.jpg\n",
      "1921 :  373 :  009645.jpg\n",
      "1922 :  374 :  005238.jpg\n",
      "1936 :  375 :  007985.jpg\n",
      "1943 :  376 :  005596.jpg\n",
      "1962 :  377 :  001585.jpg\n",
      "1987 :  378 :  009930.jpg\n",
      "1989 :  379 :  008363.jpg\n",
      "1991 :  380 :  008184.jpg\n",
      "2006 :  381 :  006126.jpg\n",
      "2027 :  382 :  006169.jpg\n",
      "2039 :  383 :  006056.jpg\n",
      "2045 :  384 :  008963.jpg\n",
      "2048 :  385 :  009651.jpg\n",
      "2051 :  386 :  004668.jpg\n",
      "2063 :  387 :  000611.jpg\n",
      "2065 :  388 :  000003.jpg\n",
      "2067 :  389 :  007404.jpg\n",
      "2070 :  390 :  008394.jpg\n",
      "2072 :  391 :  000749.jpg\n",
      "2079 :  392 :  008520.jpg\n",
      "2085 :  393 :  001377.jpg\n",
      "2090 :  394 :  002787.jpg\n",
      "2095 :  395 :  005555.jpg\n",
      "2097 :  396 :  005936.jpg\n",
      "2099 :  397 :  001568.jpg\n",
      "2101 :  398 :  008803.jpg\n",
      "2102 :  399 :  004078.jpg\n",
      "2103 :  400 :  003823.jpg\n",
      "2106 :  401 :  000280.jpg\n",
      "2108 :  402 :  005106.jpg\n",
      "2109 :  403 :  004497.jpg\n",
      "2110 :  404 :  007096.jpg\n",
      "2129 :  405 :  009164.jpg\n",
      "2136 :  406 :  009095.jpg\n",
      "2140 :  407 :  006086.jpg\n",
      "2145 :  408 :  001046.jpg\n",
      "2152 :  409 :  005623.jpg\n",
      "2163 :  410 :  001631.jpg\n",
      "2174 :  411 :  004268.jpg\n",
      "2180 :  412 :  003950.jpg\n",
      "2185 :  413 :  002026.jpg\n",
      "2187 :  414 :  006745.jpg\n",
      "2198 :  415 :  004533.jpg\n",
      "2199 :  416 :  007164.jpg\n",
      "2204 :  417 :  000116.jpg\n",
      "2206 :  418 :  007456.jpg\n",
      "2214 :  419 :  009928.jpg\n",
      "2216 :  420 :  001513.jpg\n",
      "2220 :  421 :  002982.jpg\n",
      "2227 :  422 :  005428.jpg\n",
      "2228 :  423 :  006452.jpg\n",
      "2238 :  424 :  006057.jpg\n",
      "2239 :  425 :  008779.jpg\n",
      "2241 :  426 :  009690.jpg\n",
      "2247 :  427 :  002065.jpg\n",
      "2251 :  428 :  007393.jpg\n",
      "2253 :  429 :  005180.jpg\n",
      "2260 :  430 :  000452.jpg\n",
      "2261 :  431 :  004906.jpg\n",
      "2263 :  432 :  009030.jpg\n",
      "2287 :  433 :  006795.jpg\n",
      "2307 :  434 :  001126.jpg\n",
      "2311 :  435 :  007567.jpg\n",
      "2316 :  436 :  002928.jpg\n",
      "2318 :  437 :  004633.jpg\n",
      "2338 :  438 :  001914.jpg\n",
      "2340 :  439 :  005498.jpg\n",
      "2349 :  440 :  007862.jpg\n",
      "2350 :  441 :  008673.jpg\n",
      "2356 :  442 :  002661.jpg\n",
      "2358 :  443 :  001602.jpg\n",
      "2375 :  444 :  004250.jpg\n",
      "2379 :  445 :  004363.jpg\n",
      "2381 :  446 :  008954.jpg\n",
      "2384 :  447 :  009914.jpg\n",
      "2393 :  448 :  002843.jpg\n",
      "2397 :  449 :  009901.jpg\n",
      "2402 :  450 :  003943.jpg\n",
      "2412 :  451 :  000067.jpg\n",
      "2421 :  452 :  002629.jpg\n",
      "2424 :  453 :  008729.jpg\n",
      "2425 :  454 :  009431.jpg\n",
      "2426 :  455 :  007532.jpg\n",
      "2429 :  456 :  000953.jpg\n",
      "2432 :  457 :  006380.jpg\n",
      "2442 :  458 :  007067.jpg\n",
      "2451 :  459 :  004477.jpg\n",
      "2457 :  460 :  007406.jpg\n",
      "2460 :  461 :  004572.jpg\n",
      "2467 :  462 :  008897.jpg\n",
      "2470 :  463 :  004680.jpg\n",
      "2471 :  464 :  002353.jpg\n",
      "2475 :  465 :  003448.jpg\n",
      "2482 :  466 :  005174.jpg\n",
      "2483 :  467 :  006646.jpg\n",
      "2499 :  468 :  009595.jpg\n",
      "2502 :  469 :  006232.jpg\n",
      "2505 :  470 :  001534.jpg\n",
      "2507 :  471 :  007315.jpg\n",
      "2509 :  472 :  007783.jpg\n",
      "2511 :  473 :  004599.jpg\n",
      "2519 :  474 :  000691.jpg\n",
      "2523 :  475 :  001990.jpg\n",
      "2531 :  476 :  002665.jpg\n",
      "2532 :  477 :  002711.jpg\n",
      "2533 :  478 :  004988.jpg\n",
      "2534 :  479 :  001811.jpg\n",
      "2538 :  480 :  009633.jpg\n",
      "2539 :  481 :  007026.jpg\n",
      "2543 :  482 :  001599.jpg\n",
      "2548 :  483 :  008050.jpg\n",
      "2550 :  484 :  007364.jpg\n",
      "2552 :  485 :  009876.jpg\n",
      "2554 :  486 :  002846.jpg\n",
      "2572 :  487 :  001189.jpg\n",
      "2574 :  488 :  005727.jpg\n",
      "2575 :  489 :  008600.jpg\n",
      "2578 :  490 :  006422.jpg\n",
      "2582 :  491 :  006093.jpg\n",
      "2591 :  492 :  000128.jpg\n",
      "2595 :  493 :  005844.jpg\n",
      "2596 :  494 :  007644.jpg\n",
      "2604 :  495 :  009741.jpg\n",
      "2607 :  496 :  009871.jpg\n",
      "2608 :  497 :  004559.jpg\n",
      "2616 :  498 :  001600.jpg\n",
      "2632 :  499 :  002822.jpg\n",
      "2637 :  500 :  006713.jpg\n",
      "2647 :  501 :  001021.jpg\n",
      "2661 :  502 :  001994.jpg\n",
      "2662 :  503 :  007255.jpg\n",
      "2667 :  504 :  000339.jpg\n",
      "2668 :  505 :  003490.jpg\n",
      "2676 :  506 :  008908.jpg\n",
      "2678 :  507 :  001295.jpg\n",
      "2680 :  508 :  002950.jpg\n",
      "2684 :  509 :  004887.jpg\n",
      "2695 :  510 :  002949.jpg\n",
      "2696 :  511 :  007708.jpg\n",
      "2699 :  512 :  008591.jpg\n",
      "2700 :  513 :  009626.jpg\n",
      "2702 :  514 :  008178.jpg\n",
      "2703 :  515 :  006003.jpg\n",
      "2706 :  516 :  008740.jpg\n",
      "2707 :  517 :  003141.jpg\n",
      "2709 :  518 :  008287.jpg\n",
      "2716 :  519 :  003853.jpg\n",
      "2718 :  520 :  002712.jpg\n",
      "2720 :  521 :  007366.jpg\n",
      "2721 :  522 :  007157.jpg\n",
      "2722 :  523 :  003650.jpg\n",
      "2725 :  524 :  002617.jpg\n",
      "2734 :  525 :  002974.jpg\n",
      "2736 :  526 :  006732.jpg\n",
      "2745 :  527 :  006533.jpg\n",
      "2760 :  528 :  005766.jpg\n",
      "2764 :  529 :  004661.jpg\n",
      "2773 :  530 :  003347.jpg\n",
      "2774 :  531 :  001080.jpg\n",
      "2777 :  532 :  006721.jpg\n",
      "2782 :  533 :  008705.jpg\n",
      "2788 :  534 :  003867.jpg\n",
      "2806 :  535 :  005661.jpg\n",
      "2807 :  536 :  008407.jpg\n",
      "2810 :  537 :  003345.jpg\n",
      "2812 :  538 :  009582.jpg\n",
      "2813 :  539 :  001762.jpg\n",
      "2814 :  540 :  006402.jpg\n",
      "2815 :  541 :  000217.jpg\n",
      "2817 :  542 :  000976.jpg\n",
      "2820 :  543 :  007937.jpg\n",
      "2822 :  544 :  006624.jpg\n",
      "2829 :  545 :  009297.jpg\n",
      "2834 :  546 :  000178.jpg\n",
      "2838 :  547 :  008486.jpg\n",
      "2842 :  548 :  002087.jpg\n",
      "2843 :  549 :  006390.jpg\n",
      "2844 :  550 :  006984.jpg\n",
      "2852 :  551 :  005276.jpg\n",
      "2856 :  552 :  002604.jpg\n",
      "2865 :  553 :  007598.jpg\n",
      "2866 :  554 :  008045.jpg\n",
      "2868 :  555 :  001868.jpg\n",
      "2873 :  556 :  003115.jpg\n",
      "2878 :  557 :  001163.jpg\n",
      "2883 :  558 :  004119.jpg\n",
      "2890 :  559 :  008414.jpg\n",
      "2896 :  560 :  000692.jpg\n",
      "2899 :  561 :  004698.jpg\n",
      "2903 :  562 :  002769.jpg\n",
      "2912 :  563 :  000185.jpg\n",
      "2915 :  564 :  008016.jpg\n",
      "2918 :  565 :  005726.jpg\n",
      "2920 :  566 :  007717.jpg\n",
      "2922 :  567 :  001246.jpg\n",
      "2928 :  568 :  006500.jpg\n",
      "2933 :  569 :  002905.jpg\n",
      "2937 :  570 :  006541.jpg\n",
      "2941 :  571 :  000377.jpg\n",
      "2951 :  572 :  008957.jpg\n",
      "2956 :  573 :  008671.jpg\n",
      "2958 :  574 :  009292.jpg\n",
      "2959 :  575 :  009109.jpg\n",
      "2962 :  576 :  005597.jpg\n",
      "2970 :  577 :  006024.jpg\n",
      "2974 :  578 :  009010.jpg\n",
      "2985 :  579 :  001489.jpg\n",
      "2986 :  580 :  005392.jpg\n",
      "2987 :  581 :  005941.jpg\n",
      "2992 :  582 :  004040.jpg\n",
      "2993 :  583 :  006461.jpg\n",
      "2994 :  584 :  008330.jpg\n",
      "3001 :  585 :  008754.jpg\n",
      "3003 :  586 :  005362.jpg\n",
      "3010 :  587 :  004314.jpg\n",
      "3012 :  588 :  004109.jpg\n",
      "3015 :  589 :  008734.jpg\n",
      "3024 :  590 :  008697.jpg\n",
      "3029 :  591 :  001354.jpg\n",
      "3030 :  592 :  009798.jpg\n",
      "3031 :  593 :  007837.jpg\n",
      "3033 :  594 :  003906.jpg\n",
      "3034 :  595 :  009514.jpg\n",
      "3035 :  596 :  008257.jpg\n",
      "3047 :  597 :  000097.jpg\n",
      "3052 :  598 :  008134.jpg\n",
      "3057 :  599 :  005412.jpg\n",
      "3060 :  600 :  007401.jpg\n",
      "3063 :  601 :  008156.jpg\n",
      "3064 :  602 :  008196.jpg\n",
      "3069 :  603 :  001812.jpg\n",
      "3071 :  604 :  003010.jpg\n",
      "3077 :  605 :  008458.jpg\n",
      "3091 :  606 :  004567.jpg\n",
      "3097 :  607 :  007220.jpg\n",
      "3101 :  608 :  000196.jpg\n",
      "3102 :  609 :  009662.jpg\n",
      "3108 :  610 :  000517.jpg\n",
      "3114 :  611 :  004546.jpg\n",
      "3125 :  612 :  001674.jpg\n",
      "3130 :  613 :  000940.jpg\n",
      "3132 :  614 :  008941.jpg\n",
      "3135 :  615 :  009083.jpg\n",
      "3139 :  616 :  002297.jpg\n",
      "3148 :  617 :  007628.jpg\n",
      "3149 :  618 :  002703.jpg\n",
      "3151 :  619 :  001975.jpg\n",
      "3153 :  620 :  000938.jpg\n",
      "3154 :  621 :  004918.jpg\n",
      "3155 :  622 :  005926.jpg\n",
      "3167 :  623 :  000366.jpg\n",
      "3168 :  624 :  006577.jpg\n",
      "3173 :  625 :  009635.jpg\n",
      "3179 :  626 :  007286.jpg\n",
      "3186 :  627 :  008073.jpg\n",
      "3189 :  628 :  007367.jpg\n",
      "3193 :  629 :  001244.jpg\n",
      "3198 :  630 :  005754.jpg\n",
      "3204 :  631 :  001373.jpg\n",
      "3205 :  632 :  004311.jpg\n",
      "3206 :  633 :  000600.jpg\n",
      "3213 :  634 :  008481.jpg\n",
      "3214 :  635 :  000668.jpg\n",
      "3216 :  636 :  009632.jpg\n",
      "3220 :  637 :  007237.jpg\n",
      "3223 :  638 :  008686.jpg\n",
      "3230 :  639 :  009815.jpg\n",
      "3233 :  640 :  001992.jpg\n",
      "3235 :  641 :  003322.jpg\n",
      "3246 :  642 :  001035.jpg\n",
      "3248 :  643 :  001621.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3250 :  644 :  006954.jpg\n",
      "3254 :  645 :  001629.jpg\n",
      "3255 :  646 :  008791.jpg\n",
      "3266 :  647 :  009853.jpg\n",
      "3269 :  648 :  009435.jpg\n",
      "3270 :  649 :  007832.jpg\n",
      "3277 :  650 :  003819.jpg\n",
      "3282 :  651 :  009824.jpg\n",
      "3284 :  652 :  002207.jpg\n",
      "3289 :  653 :  003776.jpg\n",
      "3292 :  654 :  004712.jpg\n",
      "3293 :  655 :  006888.jpg\n",
      "3297 :  656 :  002560.jpg\n",
      "3301 :  657 :  002707.jpg\n",
      "3306 :  658 :  007785.jpg\n",
      "3314 :  659 :  008751.jpg\n",
      "3320 :  660 :  005279.jpg\n",
      "3321 :  661 :  005976.jpg\n",
      "3333 :  662 :  006274.jpg\n",
      "3342 :  663 :  006752.jpg\n",
      "3352 :  664 :  003067.jpg\n",
      "3358 :  665 :  000327.jpg\n",
      "3360 :  666 :  009478.jpg\n",
      "3370 :  667 :  006895.jpg\n",
      "3385 :  668 :  001025.jpg\n",
      "3397 :  669 :  006195.jpg\n",
      "3419 :  670 :  001105.jpg\n",
      "3423 :  671 :  002809.jpg\n",
      "3428 :  672 :  000652.jpg\n",
      "3437 :  673 :  001996.jpg\n",
      "3439 :  674 :  009329.jpg\n",
      "3440 :  675 :  007391.jpg\n",
      "3446 :  676 :  001720.jpg\n",
      "3451 :  677 :  005464.jpg\n",
      "3457 :  678 :  005491.jpg\n",
      "3467 :  679 :  003049.jpg\n",
      "3472 :  680 :  000157.jpg\n",
      "3485 :  681 :  003707.jpg\n",
      "3493 :  682 :  005402.jpg\n",
      "3496 :  683 :  003297.jpg\n",
      "3500 :  684 :  006145.jpg\n",
      "3503 :  685 :  009452.jpg\n",
      "3506 :  686 :  000059.jpg\n",
      "3512 :  687 :  004819.jpg\n",
      "3513 :  688 :  003206.jpg\n",
      "3514 :  689 :  009167.jpg\n",
      "3519 :  690 :  006226.jpg\n",
      "3521 :  691 :  007817.jpg\n",
      "3551 :  692 :  001884.jpg\n",
      "3553 :  693 :  004056.jpg\n",
      "3554 :  694 :  002081.jpg\n",
      "3557 :  695 :  002240.jpg\n",
      "3564 :  696 :  000905.jpg\n",
      "3568 :  697 :  000260.jpg\n",
      "3575 :  698 :  001670.jpg\n",
      "3582 :  699 :  009521.jpg\n",
      "3586 :  700 :  006072.jpg\n",
      "3591 :  701 :  007472.jpg\n",
      "3600 :  702 :  008887.jpg\n",
      "3601 :  703 :  007494.jpg\n",
      "3602 :  704 :  006010.jpg\n",
      "3605 :  705 :  002754.jpg\n",
      "3613 :  706 :  009444.jpg\n",
      "3615 :  707 :  003445.jpg\n",
      "3617 :  708 :  009262.jpg\n",
      "3623 :  709 :  007301.jpg\n",
      "3629 :  710 :  000335.jpg\n",
      "3644 :  711 :  007778.jpg\n",
      "3652 :  712 :  004128.jpg\n",
      "3674 :  713 :  005096.jpg\n",
      "3684 :  714 :  003541.jpg\n",
      "3686 :  715 :  003738.jpg\n",
      "3687 :  716 :  003323.jpg\n",
      "3698 :  717 :  007989.jpg\n",
      "3699 :  718 :  005898.jpg\n",
      "3701 :  719 :  002951.jpg\n",
      "3705 :  720 :  005234.jpg\n",
      "3707 :  721 :  007711.jpg\n",
      "3710 :  722 :  004449.jpg\n",
      "3713 :  723 :  007648.jpg\n",
      "3715 :  724 :  001929.jpg\n",
      "3728 :  725 :  004556.jpg\n",
      "3732 :  726 :  001805.jpg\n",
      "3733 :  727 :  006477.jpg\n",
      "3743 :  728 :  007504.jpg\n",
      "3747 :  729 :  000618.jpg\n",
      "3762 :  730 :  000886.jpg\n",
      "3771 :  731 :  007225.jpg\n",
      "3774 :  732 :  007257.jpg\n",
      "3776 :  733 :  002506.jpg\n",
      "3782 :  734 :  000299.jpg\n",
      "3793 :  735 :  003881.jpg\n",
      "3796 :  736 :  001167.jpg\n",
      "3803 :  737 :  001957.jpg\n",
      "3818 :  738 :  007756.jpg\n",
      "3823 :  739 :  008382.jpg\n",
      "3832 :  740 :  004422.jpg\n",
      "3835 :  741 :  005442.jpg\n",
      "3837 :  742 :  009118.jpg\n",
      "3840 :  743 :  000247.jpg\n",
      "3842 :  744 :  005266.jpg\n",
      "3843 :  745 :  007816.jpg\n",
      "3844 :  746 :  009606.jpg\n",
      "3848 :  747 :  005060.jpg\n",
      "3852 :  748 :  001773.jpg\n",
      "3870 :  749 :  003825.jpg\n",
      "3871 :  750 :  003381.jpg\n",
      "3875 :  751 :  000202.jpg\n",
      "3878 :  752 :  008347.jpg\n",
      "3883 :  753 :  007739.jpg\n",
      "3894 :  754 :  007442.jpg\n",
      "3901 :  755 :  009630.jpg\n",
      "3902 :  756 :  002650.jpg\n",
      "3904 :  757 :  001429.jpg\n",
      "3907 :  758 :  008094.jpg\n",
      "3911 :  759 :  006676.jpg\n",
      "3914 :  760 :  002945.jpg\n",
      "3915 :  761 :  003494.jpg\n",
      "3918 :  762 :  007835.jpg\n",
      "3921 :  763 :  005627.jpg\n",
      "3925 :  764 :  000547.jpg\n",
      "3926 :  765 :  008493.jpg\n",
      "3927 :  766 :  007492.jpg\n",
      "3938 :  767 :  004398.jpg\n",
      "3942 :  768 :  009225.jpg\n",
      "3943 :  769 :  002894.jpg\n",
      "3953 :  770 :  000974.jpg\n",
      "3960 :  771 :  003725.jpg\n",
      "3967 :  772 :  004162.jpg\n",
      "3972 :  773 :  008459.jpg\n",
      "3976 :  774 :  003902.jpg\n",
      "3977 :  775 :  004068.jpg\n",
      "3980 :  776 :  001340.jpg\n",
      "3984 :  777 :  008041.jpg\n",
      "3986 :  778 :  003037.jpg\n",
      "3988 :  779 :  008902.jpg\n",
      "3998 :  780 :  004476.jpg\n",
      "4005 :  781 :  007870.jpg\n",
      "4021 :  782 :  006705.jpg\n",
      "4035 :  783 :  004153.jpg\n",
      "4037 :  784 :  000084.jpg\n",
      "4041 :  785 :  000968.jpg\n",
      "4048 :  786 :  004862.jpg\n",
      "4049 :  787 :  000487.jpg\n",
      "4052 :  788 :  001179.jpg\n",
      "4058 :  789 :  001276.jpg\n",
      "4062 :  790 :  009689.jpg\n",
      "4069 :  791 :  006630.jpg\n",
      "4070 :  792 :  004791.jpg\n",
      "4072 :  793 :  002619.jpg\n",
      "4073 :  794 :  001652.jpg\n",
      "4076 :  795 :  001351.jpg\n",
      "4078 :  796 :  001302.jpg\n",
      "4079 :  797 :  003526.jpg\n",
      "4082 :  798 :  004278.jpg\n",
      "4102 :  799 :  000006.jpg\n",
      "4103 :  800 :  001551.jpg\n",
      "4104 :  801 :  004355.jpg\n",
      "4107 :  802 :  004620.jpg\n",
      "4108 :  803 :  007508.jpg\n",
      "4112 :  804 :  003139.jpg\n",
      "4113 :  805 :  004166.jpg\n",
      "4118 :  806 :  009919.jpg\n",
      "4131 :  807 :  002414.jpg\n",
      "4139 :  808 :  003676.jpg\n",
      "4144 :  809 :  005589.jpg\n",
      "4149 :  810 :  003446.jpg\n",
      "4150 :  811 :  002663.jpg\n",
      "4153 :  812 :  005635.jpg\n",
      "4157 :  813 :  008554.jpg\n",
      "4168 :  814 :  005458.jpg\n",
      "4169 :  815 :  004118.jpg\n",
      "4176 :  816 :  001177.jpg\n",
      "4177 :  817 :  001261.jpg\n",
      "4178 :  818 :  003944.jpg\n",
      "4190 :  819 :  004199.jpg\n",
      "4192 :  820 :  002788.jpg\n",
      "4194 :  821 :  004180.jpg\n",
      "4211 :  822 :  002429.jpg\n",
      "4213 :  823 :  007825.jpg\n",
      "4216 :  824 :  008543.jpg\n",
      "4219 :  825 :  006050.jpg\n",
      "4221 :  826 :  002581.jpg\n",
      "4223 :  827 :  000226.jpg\n",
      "4235 :  828 :  008182.jpg\n",
      "4237 :  829 :  006792.jpg\n",
      "4238 :  830 :  004042.jpg\n",
      "4248 :  831 :  000981.jpg\n",
      "4267 :  832 :  003160.jpg\n",
      "4268 :  833 :  002773.jpg\n",
      "4275 :  834 :  002322.jpg\n",
      "4277 :  835 :  009294.jpg\n",
      "4281 :  836 :  000243.jpg\n",
      "4285 :  837 :  007240.jpg\n",
      "4288 :  838 :  007012.jpg\n",
      "4290 :  839 :  008233.jpg\n",
      "4308 :  840 :  005633.jpg\n",
      "4311 :  841 :  001912.jpg\n",
      "4321 :  842 :  002159.jpg\n",
      "4330 :  843 :  003931.jpg\n",
      "4331 :  844 :  001349.jpg\n",
      "4346 :  845 :  007110.jpg\n",
      "4349 :  846 :  009055.jpg\n",
      "4357 :  847 :  000659.jpg\n",
      "4361 :  848 :  001419.jpg\n",
      "4366 :  849 :  003438.jpg\n",
      "4367 :  850 :  003249.jpg\n",
      "4369 :  851 :  005575.jpg\n",
      "4375 :  852 :  006307.jpg\n",
      "4376 :  853 :  006347.jpg\n",
      "4377 :  854 :  007384.jpg\n",
      "4378 :  855 :  003076.jpg\n",
      "4383 :  856 :  002389.jpg\n",
      "4387 :  857 :  004061.jpg\n",
      "4391 :  858 :  006152.jpg\n",
      "4397 :  859 :  005767.jpg\n",
      "4399 :  860 :  001138.jpg\n",
      "4417 :  861 :  009487.jpg\n",
      "4420 :  862 :  009511.jpg\n",
      "4424 :  863 :  006451.jpg\n",
      "4429 :  864 :  008844.jpg\n",
      "4431 :  865 :  009660.jpg\n",
      "4435 :  866 :  001850.jpg\n",
      "4441 :  867 :  003319.jpg\n",
      "4449 :  868 :  005934.jpg\n",
      "4452 :  869 :  000737.jpg\n",
      "4467 :  870 :  001848.jpg\n",
      "4472 :  871 :  004881.jpg\n",
      "4489 :  872 :  003697.jpg\n",
      "4502 :  873 :  009313.jpg\n",
      "4503 :  874 :  006457.jpg\n",
      "4504 :  875 :  007744.jpg\n",
      "4514 :  876 :  002014.jpg\n",
      "4518 :  877 :  009838.jpg\n",
      "4535 :  878 :  000641.jpg\n",
      "4545 :  879 :  008626.jpg\n",
      "4549 :  880 :  000893.jpg\n",
      "4551 :  881 :  005665.jpg\n",
      "4559 :  882 :  009257.jpg\n",
      "4561 :  883 :  001023.jpg\n",
      "4574 :  884 :  002862.jpg\n",
      "4584 :  885 :  008583.jpg\n",
      "4593 :  886 :  002189.jpg\n",
      "4601 :  887 :  004720.jpg\n",
      "4608 :  888 :  005376.jpg\n",
      "4621 :  889 :  003928.jpg\n",
      "4622 :  890 :  002538.jpg\n",
      "4653 :  891 :  002052.jpg\n",
      "4655 :  892 :  002141.jpg\n",
      "4671 :  893 :  006271.jpg\n",
      "4673 :  894 :  009547.jpg\n",
      "4675 :  895 :  005545.jpg\n",
      "4676 :  896 :  003314.jpg\n",
      "4686 :  897 :  002089.jpg\n",
      "4716 :  898 :  008528.jpg\n",
      "4723 :  899 :  001139.jpg\n",
      "4734 :  900 :  003309.jpg\n",
      "4742 :  901 :  006946.jpg\n",
      "4745 :  902 :  002610.jpg\n",
      "4753 :  903 :  004784.jpg\n",
      "4756 :  904 :  004721.jpg\n",
      "4758 :  905 :  002107.jpg\n",
      "4760 :  906 :  002656.jpg\n",
      "4767 :  907 :  005858.jpg\n",
      "4770 :  908 :  004064.jpg\n",
      "4773 :  909 :  009310.jpg\n",
      "4774 :  910 :  004880.jpg\n",
      "4776 :  911 :  002614.jpg\n",
      "4781 :  912 :  001223.jpg\n",
      "4785 :  913 :  008938.jpg\n",
      "4797 :  914 :  004366.jpg\n",
      "4798 :  915 :  007319.jpg\n",
      "4800 :  916 :  007155.jpg\n",
      "4803 :  917 :  004134.jpg\n",
      "4817 :  918 :  005034.jpg\n",
      "4818 :  919 :  005295.jpg\n",
      "4820 :  920 :  005184.jpg\n",
      "4824 :  921 :  006774.jpg\n",
      "4825 :  922 :  000825.jpg\n",
      "4830 :  923 :  002575.jpg\n",
      "4834 :  924 :  007502.jpg\n",
      "4849 :  925 :  002552.jpg\n",
      "4851 :  926 :  005703.jpg\n",
      "4853 :  927 :  007995.jpg\n",
      "4854 :  928 :  000447.jpg\n",
      "4855 :  929 :  000346.jpg\n",
      "4856 :  930 :  008864.jpg\n",
      "4859 :  931 :  008055.jpg\n",
      "4863 :  932 :  006644.jpg\n",
      "4870 :  933 :  003643.jpg\n",
      "4882 :  934 :  000595.jpg\n",
      "4892 :  935 :  001609.jpg\n",
      "4894 :  936 :  006081.jpg\n",
      "4895 :  937 :  008555.jpg\n",
      "4896 :  938 :  004892.jpg\n",
      "4899 :  939 :  003582.jpg\n",
      "4914 :  940 :  007352.jpg\n",
      "4937 :  941 :  009854.jpg\n",
      "4943 :  942 :  009840.jpg\n",
      "4946 :  943 :  004491.jpg\n",
      "4947 :  944 :  006663.jpg\n",
      "4949 :  945 :  006393.jpg\n",
      "4950 :  946 :  004236.jpg\n",
      "Average Precision for class aeroplane is 0.332278481013\n",
      "Correctly predicted images for class aeroplane are [u'009292.jpg', u'002851.jpg', u'001850.jpg', u'002246.jpg', u'007711.jpg', u'002665.jpg', u'008440.jpg', u'005076.jpg', u'007026.jpg', u'008554.jpg', u'009919.jpg', u'002619.jpg', u'001305.jpg', u'008352.jpg', u'007169.jpg', u'002052.jpg', u'007157.jpg', u'004680.jpg', u'005965.jpg', u'006888.jpg', u'002754.jpg', u'001848.jpg', u'002217.jpg', u'007995.jpg', u'007825.jpg', u'001885.jpg', u'006024.jpg', u'002198.jpg', u'001621.jpg', u'000067.jpg', u'009876.jpg', u'007267.jpg', u'004199.jpg', u'004078.jpg', u'002467.jpg', u'000696.jpg', u'001547.jpg', u'008986.jpg', u'009262.jpg', u'004314.jpg', u'007096.jpg']\n",
      "Average Precision for class bottle is 0.121428571429\n",
      "Correctly predicted images for class bottle are [u'000369.jpg', u'008134.jpg', u'000762.jpg', u'009547.jpg', u'003431.jpg', u'008134.jpg', u'001295.jpg', u'002711.jpg', u'004236.jpg', u'008287.jpg', u'001431.jpg', u'000447.jpg', u'008134.jpg', u'006500.jpg', u'001429.jpg', u'004236.jpg', u'006003.jpg']\n",
      "Average Precision for class chair is 0.0559921414538\n",
      "Correctly predicted images for class chair are [u'007301.jpg', u'003345.jpg', u'006646.jpg', u'002240.jpg', u'009631.jpg', u'008382.jpg', u'002449.jpg', u'005412.jpg', u'002850.jpg', u'005193.jpg', u'005898.jpg', u'009854.jpg', u'009741.jpg', u'003520.jpg', u'006010.jpg', u'000196.jpg', u'004449.jpg', u'004040.jpg', u'008963.jpg', u'009118.jpg', u'002788.jpg', u'006126.jpg', u'008947.jpg', u'000006.jpg', u'009355.jpg', u'000377.jpg', u'005313.jpg', u'008922.jpg', u'002788.jpg', u'002161.jpg', u'009355.jpg', u'003049.jpg', u'009853.jpg', u'006142.jpg', u'006393.jpg', u'009854.jpg', u'001489.jpg', u'003902.jpg', u'005180.jpg', u'006057.jpg', u'004139.jpg', u'000006.jpg', u'005665.jpg', u'007067.jpg', u'004319.jpg', u'009677.jpg', u'009635.jpg', u'002930.jpg', u'002930.jpg', u'004744.jpg', u'009741.jpg', u'003676.jpg', u'001179.jpg', u'008113.jpg', u'001276.jpg', u'009854.jpg', u'000510.jpg', u'009662.jpg', u'002638.jpg', u'002389.jpg', u'004525.jpg', u'002301.jpg', u'001244.jpg', u'004486.jpg', u'004862.jpg', u'003141.jpg', u'003245.jpg', u'000621.jpg', u'007268.jpg', u'005589.jpg', u'005934.jpg', u'001039.jpg', u'002353.jpg', u'009690.jpg', u'005734.jpg', u'009514.jpg', u'005665.jpg', u'002792.jpg', u'000217.jpg', u'004640.jpg', u'003141.jpg', u'009663.jpg', u'005665.jpg', u'005941.jpg', u'001302.jpg', u'001276.jpg', u'001163.jpg', u'000366.jpg', u'001138.jpg', u'003139.jpg', u'000510.jpg', u'001990.jpg', u'008113.jpg', u'001868.jpg', u'002159.jpg', u'008941.jpg', u'005295.jpg', u'005665.jpg', u'000084.jpg', u'009853.jpg', u'000385.jpg', u'001244.jpg', u'009815.jpg', u'001990.jpg', u'003323.jpg', u'008257.jpg', u'006461.jpg', u'001139.jpg', u'009397.jpg', u'005044.jpg', u'000953.jpg', u'003502.jpg', u'007255.jpg', u'003502.jpg']\n",
      "MAP: 0.169899731298\n",
      "CPU times: user 9h 42min 4s, sys: 1h 3s, total: 10h 42min 8s\n",
      "Wall time: 5h 28min 36s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.16989973129835356"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time test(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two Layer Detection\n",
    "def test_two_layer(model):\n",
    "    model_wts = torch.load('two_layer_v3/two_layer_best_model.wts')\n",
    "    # load best model weights\n",
    "    model.load_state_dict(model_wts)\n",
    "    model.eval()\n",
    "    print(\"Model Loaded\")\n",
    "    \n",
    "    pred_bndboxes = [[],[],[],[]]\n",
    "    actual = {}\n",
    "    pr = [[],[],[],[]]  \n",
    "    re = [[],[],[],[]]  \n",
    "    \n",
    "    # function to predict the class of given images, using our trained model\n",
    "    def predict_class(inputs):\n",
    "        # get the raw output from the model\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(Variable(inputs))\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        preds = preds.tolist()\n",
    "        # convert to probability scores\n",
    "        prob_layer = nn.Softmax(dim=1)\n",
    "        probs = prob_layer(outputs).tolist()\n",
    "        out_probs = np.amax(probs, axis=1)\n",
    "        # return bbox along with the prob score\n",
    "        return list(zip(preds, out_probs))\n",
    "    \n",
    "    # print the bBoxes on the image\n",
    "    def print_boxes(img, bbs, color):\n",
    "        for bb in bbs:\n",
    "            cv2.rectangle(img, (int(bb[1]), int(bb[2])), (int(bb[3]), int(bb[4])), color, 2)\n",
    "        return img\n",
    "        \n",
    "    count = 0\n",
    "    allowed_labels = ['aeroplane', 'bottle', 'chair']\n",
    "    label_counts = [0,0,0,0]\n",
    "    test_images_folder = os.path.join('./test', 'VOC2007/JPEGImages/')\n",
    "    test_objects_folder = os.path.join('./test', 'VOC2007/Annotations/')\n",
    "    # read the test images\n",
    "    for idx, image_name in enumerate(os.listdir(test_images_folder)):\n",
    "        image_file = os.path.join(test_images_folder, image_name)\n",
    "        objects_file = os.path.join(test_objects_folder, image_name.split(\".\")[0]+'.xml')\n",
    "        tree = ET.parse(objects_file)\n",
    "        img = Image.open(image_file).convert('RGB')\n",
    "        root = tree.getroot()\n",
    "        labels = []\n",
    "        bndboxes = []\n",
    "        for obj in root.findall('object'):\n",
    "            label = obj.find('name').text\n",
    "            if label not in allowed_labels:\n",
    "                continue\n",
    "            label_counts[classes.index(label)] += 1\n",
    "            bndbox = obj.find('bndbox')\n",
    "            coord = []\n",
    "            for cd in bndbox:\n",
    "                coord.append(int(cd.text))\n",
    "            labels.append(label)\n",
    "            bndboxes.append(coord)\n",
    "        \n",
    "        # ignore the image if no object present in it\n",
    "        if len(labels) == 0:\n",
    "            continue\n",
    "        count = count + 1\n",
    "        print(idx+1, \": \", count, \": \", image_name)\n",
    "    \n",
    "        # run the sliding window approach to extract all possible bounding boxes with objects\n",
    "        boxes_a, boxes_b, boxes_c = sliding_window(img, model, predict_class, sthreshold=0.85)\n",
    "        \n",
    "        # run NMS separately on all boxes of a specific class to get the reduced number of boxes\n",
    "        pred_boxes = [[],[],[],[]]\n",
    "        pred_boxes[classes.index('aeroplane')] = non_maximum_supression(boxes_a, image_name)\n",
    "        pred_boxes[classes.index('bottle')] = non_maximum_supression(boxes_b, image_name)\n",
    "        pred_boxes[classes.index('chair')] = non_maximum_supression(boxes_c, image_name)\n",
    "             \n",
    "        # save the predicted object Bboxes in the image to the global list\n",
    "        for idx, class_name in enumerate(classes):\n",
    "            if class_name == '__background__':\n",
    "                continue\n",
    "            pred_bndboxes[idx] = pred_bndboxes[idx] + pred_boxes[idx]\n",
    "        \n",
    "        # print the bBoxes on the image and store it\n",
    "        img = cv2.imread(image_file)\n",
    "        img = print_boxes(img, pred_boxes[classes.index('aeroplane')], (255,0,0)) #blue \n",
    "        img = print_boxes(img, pred_boxes[classes.index('bottle')], (0,255,0))    #green\n",
    "        img = print_boxes(img, pred_boxes[classes.index('chair')], (0,0,255))     #red\n",
    "        cv2.imwrite(\"outputs/\"+image_name, img)\n",
    "        actual[image_name] = [[],[],[],[]]\n",
    "        \n",
    "        # save the actual object bBoxes in the image to the global\n",
    "        for lbl, bd in zip(labels, bndboxes):\n",
    "            actual[image_name][classes.index(lbl)].append(bd)\n",
    "        \n",
    "    # calculate the precision and recall vectors for each class\n",
    "    correct = [[],[],[],[]]\n",
    "    for idx, class_name in enumerate(classes):\n",
    "        if class_name == '__background__':\n",
    "            continue\n",
    "        pr[idx], re[idx], correct[idx] = calculate_precision_recall(pred_bndboxes[idx], actual, idx, label_counts[idx])\n",
    "    \n",
    "    # get the Average precision for each class\n",
    "    ap = [0, 0, 0, 0]\n",
    "    for idx, class_name in enumerate(classes):\n",
    "        if class_name == '__background__':\n",
    "            continue\n",
    "        # print(pr[idx],re[idx])\n",
    "        ap[idx] = calculate_average_precision(pr[idx], re[idx])\n",
    "        \n",
    "    \n",
    "    # print the average precision for all classes\n",
    "    # and the correctly predicted images for all classes\n",
    "    ap_sum = 0.0\n",
    "    for idx, class_name in enumerate(classes):\n",
    "        if class_name == '__background__':\n",
    "            continue\n",
    "        print(\"Average Precision for class \"+class_name+\" is \"+str(ap[idx]))\n",
    "        print(\"Correctly predicted images for class \"+class_name+\" are \"+str(correct[idx]))\n",
    "        ap_sum += ap[idx]\n",
    "         \n",
    "    # get the mAP score as the average of AP of all classes\n",
    "    mAP = ap_sum/(len(ap)-1)\n",
    "    \n",
    "    print(\"MAP:\",mAP)\n",
    "    return mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n",
      "9 :  1 :  009570.jpg\n",
      "10 :  2 :  004055.jpg\n",
      "16 :  3 :  004964.jpg\n",
      "23 :  4 :  005537.jpg\n",
      "29 :  5 :  000573.jpg\n",
      "36 :  6 :  005622.jpg\n",
      "53 :  7 :  009742.jpg\n",
      "54 :  8 :  001921.jpg\n",
      "56 :  9 :  005725.jpg\n",
      "72 :  10 :  007348.jpg\n",
      "74 :  11 :  005043.jpg\n",
      "77 :  12 :  002907.jpg\n",
      "81 :  13 :  000316.jpg\n",
      "86 :  14 :  007738.jpg\n",
      "89 :  15 :  005907.jpg\n",
      "92 :  16 :  006263.jpg\n",
      "93 :  17 :  009012.jpg\n",
      "99 :  18 :  009663.jpg\n",
      "102 :  19 :  008131.jpg\n",
      "103 :  20 :  004262.jpg\n",
      "117 :  21 :  005827.jpg\n",
      "119 :  22 :  003544.jpg\n",
      "121 :  23 :  004744.jpg\n",
      "126 :  24 :  005286.jpg\n",
      "127 :  25 :  005218.jpg\n",
      "128 :  26 :  007564.jpg\n",
      "137 :  27 :  000277.jpg\n",
      "142 :  28 :  000642.jpg\n",
      "151 :  29 :  005103.jpg\n",
      "152 :  30 :  003649.jpg\n",
      "157 :  31 :  008646.jpg\n",
      "159 :  32 :  005050.jpg\n",
      "163 :  33 :  002583.jpg\n",
      "165 :  34 :  005474.jpg\n",
      "175 :  35 :  009846.jpg\n",
      "179 :  36 :  008110.jpg\n",
      "195 :  37 :  009076.jpg\n",
      "206 :  38 :  005994.jpg\n",
      "211 :  39 :  003532.jpg\n",
      "218 :  40 :  000696.jpg\n",
      "225 :  41 :  007698.jpg\n",
      "227 :  42 :  006780.jpg\n",
      "229 :  43 :  009311.jpg\n",
      "235 :  44 :  005040.jpg\n",
      "241 :  45 :  004377.jpg\n",
      "250 :  46 :  005196.jpg\n",
      "252 :  47 :  004923.jpg\n",
      "257 :  48 :  002007.jpg\n",
      "258 :  49 :  000124.jpg\n",
      "263 :  50 :  008986.jpg\n",
      "268 :  51 :  003761.jpg\n",
      "269 :  52 :  007973.jpg\n",
      "271 :  53 :  001433.jpg\n",
      "286 :  54 :  008991.jpg\n",
      "292 :  55 :  003842.jpg\n",
      "293 :  56 :  001305.jpg\n",
      "299 :  57 :  008113.jpg\n",
      "307 :  58 :  009171.jpg\n",
      "308 :  59 :  001712.jpg\n",
      "310 :  60 :  001660.jpg\n",
      "324 :  61 :  003736.jpg\n",
      "325 :  62 :  008627.jpg\n",
      "336 :  63 :  005076.jpg\n",
      "345 :  64 :  009962.jpg\n",
      "349 :  65 :  001829.jpg\n",
      "350 :  66 :  006297.jpg\n",
      "354 :  67 :  007722.jpg\n",
      "366 :  68 :  000397.jpg\n",
      "371 :  69 :  002357.jpg\n",
      "373 :  70 :  006293.jpg\n",
      "376 :  71 :  005008.jpg\n",
      "381 :  72 :  002705.jpg\n",
      "394 :  73 :  008245.jpg\n",
      "397 :  74 :  009835.jpg\n",
      "405 :  75 :  005734.jpg\n",
      "410 :  76 :  000090.jpg\n",
      "411 :  77 :  005721.jpg\n",
      "414 :  78 :  006360.jpg\n",
      "416 :  79 :  004348.jpg\n",
      "419 :  80 :  004227.jpg\n",
      "420 :  81 :  008039.jpg\n",
      "423 :  82 :  008431.jpg\n",
      "429 :  83 :  004820.jpg\n",
      "433 :  84 :  005927.jpg\n",
      "438 :  85 :  008778.jpg\n",
      "444 :  86 :  003770.jpg\n",
      "445 :  87 :  004780.jpg\n",
      "460 :  88 :  008950.jpg\n",
      "461 :  89 :  002644.jpg\n",
      "463 :  90 :  002813.jpg\n",
      "464 :  91 :  001519.jpg\n",
      "466 :  92 :  009646.jpg\n",
      "477 :  93 :  008761.jpg\n",
      "480 :  94 :  001569.jpg\n",
      "489 :  95 :  002100.jpg\n",
      "490 :  96 :  008821.jpg\n",
      "492 :  97 :  003341.jpg\n",
      "498 :  98 :  005002.jpg\n",
      "499 :  99 :  003755.jpg\n",
      "505 :  100 :  007548.jpg\n",
      "506 :  101 :  007747.jpg\n",
      "517 :  102 :  003942.jpg\n",
      "524 :  103 :  002908.jpg\n",
      "530 :  104 :  009486.jpg\n",
      "535 :  105 :  001696.jpg\n",
      "537 :  106 :  001925.jpg\n",
      "543 :  107 :  002167.jpg\n",
      "546 :  108 :  008496.jpg\n",
      "550 :  109 :  005745.jpg\n",
      "563 :  110 :  008714.jpg\n",
      "564 :  111 :  002499.jpg\n",
      "566 :  112 :  007178.jpg\n",
      "568 :  113 :  006826.jpg\n",
      "569 :  114 :  007684.jpg\n",
      "571 :  115 :  009677.jpg\n",
      "578 :  116 :  000560.jpg\n",
      "580 :  117 :  002510.jpg\n",
      "588 :  118 :  008400.jpg\n",
      "589 :  119 :  007403.jpg\n",
      "590 :  120 :  001319.jpg\n",
      "605 :  121 :  008947.jpg\n",
      "608 :  122 :  002217.jpg\n",
      "609 :  123 :  005249.jpg\n",
      "620 :  124 :  000587.jpg\n",
      "631 :  125 :  008192.jpg\n",
      "632 :  126 :  000745.jpg\n",
      "646 :  127 :  004098.jpg\n",
      "650 :  128 :  005942.jpg\n",
      "656 :  129 :  008379.jpg\n",
      "659 :  130 :  001856.jpg\n",
      "661 :  131 :  009332.jpg\n",
      "663 :  132 :  009366.jpg\n",
      "664 :  133 :  000994.jpg\n",
      "672 :  134 :  000621.jpg\n",
      "673 :  135 :  002246.jpg\n",
      "677 :  136 :  007228.jpg\n",
      "681 :  137 :  002231.jpg\n",
      "683 :  138 :  004469.jpg\n",
      "703 :  139 :  003268.jpg\n",
      "707 :  140 :  001735.jpg\n",
      "719 :  141 :  008490.jpg\n",
      "720 :  142 :  001547.jpg\n",
      "739 :  143 :  006514.jpg\n",
      "743 :  144 :  007135.jpg\n",
      "758 :  145 :  008825.jpg\n",
      "764 :  146 :  004717.jpg\n",
      "766 :  147 :  001657.jpg\n",
      "767 :  148 :  007896.jpg\n",
      "769 :  149 :  001039.jpg\n",
      "777 :  150 :  002264.jpg\n",
      "781 :  151 :  000144.jpg\n",
      "782 :  152 :  005965.jpg\n",
      "790 :  153 :  001087.jpg\n",
      "806 :  154 :  008210.jpg\n",
      "811 :  155 :  003245.jpg\n",
      "814 :  156 :  009084.jpg\n",
      "820 :  157 :  008922.jpg\n",
      "827 :  158 :  004506.jpg\n",
      "832 :  159 :  007632.jpg\n",
      "837 :  160 :  007354.jpg\n",
      "838 :  161 :  008881.jpg\n",
      "840 :  162 :  003502.jpg\n",
      "842 :  163 :  002885.jpg\n",
      "845 :  164 :  004677.jpg\n",
      "847 :  165 :  007993.jpg\n",
      "849 :  166 :  001826.jpg\n",
      "851 :  167 :  007290.jpg\n",
      "868 :  168 :  006651.jpg\n",
      "871 :  169 :  002851.jpg\n",
      "880 :  170 :  002298.jpg\n",
      "890 :  171 :  003230.jpg\n",
      "891 :  172 :  006294.jpg\n",
      "896 :  173 :  003590.jpg\n",
      "897 :  174 :  004971.jpg\n",
      "901 :  175 :  004919.jpg\n",
      "906 :  176 :  004053.jpg\n",
      "910 :  177 :  003478.jpg\n",
      "911 :  178 :  004486.jpg\n",
      "913 :  179 :  008669.jpg\n",
      "919 :  180 :  007203.jpg\n",
      "920 :  181 :  002857.jpg\n",
      "922 :  182 :  000910.jpg\n",
      "930 :  183 :  004139.jpg\n",
      "951 :  184 :  002971.jpg\n",
      "954 :  185 :  009529.jpg\n",
      "967 :  186 :  001407.jpg\n",
      "975 :  187 :  006743.jpg\n",
      "976 :  188 :  006432.jpg\n",
      "977 :  189 :  004922.jpg\n",
      "985 :  190 :  005570.jpg\n",
      "994 :  191 :  001153.jpg\n",
      "995 :  192 :  003520.jpg\n",
      "1002 :  193 :  002753.jpg\n",
      "1010 :  194 :  008056.jpg\n",
      "1014 :  195 :  004127.jpg\n",
      "1016 :  196 :  005556.jpg\n",
      "1019 :  197 :  009916.jpg\n",
      "1024 :  198 :  008516.jpg\n",
      "1029 :  199 :  002365.jpg\n",
      "1036 :  200 :  002792.jpg\n",
      "1038 :  201 :  001081.jpg\n",
      "1041 :  202 :  003617.jpg\n",
      "1042 :  203 :  005044.jpg\n",
      "1046 :  204 :  008925.jpg\n",
      "1048 :  205 :  000111.jpg\n",
      "1049 :  206 :  000385.jpg\n",
      "1052 :  207 :  007752.jpg\n",
      "1055 :  208 :  004716.jpg\n",
      "1058 :  209 :  005233.jpg\n",
      "1062 :  210 :  000521.jpg\n",
      "1063 :  211 :  008537.jpg\n",
      "1066 :  212 :  005291.jpg\n",
      "1076 :  213 :  004525.jpg\n",
      "1078 :  214 :  006359.jpg\n",
      "1081 :  215 :  008678.jpg\n",
      "1088 :  216 :  002724.jpg\n",
      "1092 :  217 :  009891.jpg\n",
      "1098 :  218 :  002674.jpg\n",
      "1103 :  219 :  001742.jpg\n",
      "1108 :  220 :  001949.jpg\n",
      "1110 :  221 :  003665.jpg\n",
      "1112 :  222 :  000473.jpg\n",
      "1115 :  223 :  008405.jpg\n",
      "1116 :  224 :  002449.jpg\n",
      "1118 :  225 :  008367.jpg\n",
      "1126 :  226 :  002904.jpg\n",
      "1127 :  227 :  006453.jpg\n",
      "1132 :  228 :  007262.jpg\n",
      "1143 :  229 :  002850.jpg\n",
      "1144 :  230 :  004781.jpg\n",
      "1158 :  231 :  009782.jpg\n",
      "1159 :  232 :  007828.jpg\n",
      "1165 :  233 :  006758.jpg\n",
      "1167 :  234 :  006633.jpg\n",
      "1196 :  235 :  002157.jpg\n",
      "1202 :  236 :  001942.jpg\n",
      "1207 :  237 :  007268.jpg\n",
      "1208 :  238 :  002198.jpg\n",
      "1213 :  239 :  008089.jpg\n",
      "1214 :  240 :  000136.jpg\n",
      "1217 :  241 :  007207.jpg\n",
      "1225 :  242 :  002110.jpg\n",
      "1233 :  243 :  001222.jpg\n",
      "1236 :  244 :  009056.jpg\n",
      "1238 :  245 :  003278.jpg\n",
      "1244 :  246 :  001715.jpg\n",
      "1247 :  247 :  008861.jpg\n",
      "1269 :  248 :  002743.jpg\n",
      "1276 :  249 :  008894.jpg\n",
      "1284 :  250 :  007057.jpg\n",
      "1287 :  251 :  002503.jpg\n",
      "1295 :  252 :  000234.jpg\n",
      "1297 :  253 :  002930.jpg\n",
      "1301 :  254 :  002536.jpg\n",
      "1304 :  255 :  004695.jpg\n",
      "1305 :  256 :  000846.jpg\n",
      "1317 :  257 :  000181.jpg\n",
      "1319 :  258 :  004319.jpg\n",
      "1320 :  259 :  007676.jpg\n",
      "1325 :  260 :  001961.jpg\n",
      "1334 :  261 :  003372.jpg\n",
      "1341 :  262 :  003802.jpg\n",
      "1342 :  263 :  007500.jpg\n",
      "1346 :  264 :  006248.jpg\n",
      "1350 :  265 :  007652.jpg\n",
      "1355 :  266 :  009075.jpg\n",
      "1360 :  267 :  009211.jpg\n",
      "1361 :  268 :  002638.jpg\n",
      "1362 :  269 :  004824.jpg\n",
      "1365 :  270 :  003071.jpg\n",
      "1369 :  271 :  007267.jpg\n",
      "1372 :  272 :  002463.jpg\n",
      "1373 :  273 :  007700.jpg\n",
      "1378 :  274 :  001055.jpg\n",
      "1386 :  275 :  009397.jpg\n",
      "1390 :  276 :  003775.jpg\n",
      "1394 :  277 :  005302.jpg\n",
      "1399 :  278 :  008474.jpg\n",
      "1400 :  279 :  009482.jpg\n",
      "1402 :  280 :  007341.jpg\n",
      "1428 :  281 :  001885.jpg\n",
      "1431 :  282 :  001456.jpg\n",
      "1442 :  283 :  002161.jpg\n",
      "1448 :  284 :  001169.jpg\n",
      "1450 :  285 :  001431.jpg\n",
      "1456 :  286 :  000314.jpg\n",
      "1457 :  287 :  009009.jpg\n",
      "1459 :  288 :  003209.jpg\n",
      "1466 :  289 :  003459.jpg\n",
      "1468 :  290 :  003384.jpg\n",
      "1476 :  291 :  009355.jpg\n",
      "1481 :  292 :  009513.jpg\n",
      "1482 :  293 :  008352.jpg\n",
      "1484 :  294 :  003894.jpg\n",
      "1486 :  295 :  002467.jpg\n",
      "1491 :  296 :  009708.jpg\n",
      "1492 :  297 :  002235.jpg\n",
      "1495 :  298 :  000126.jpg\n",
      "1500 :  299 :  004640.jpg\n",
      "1501 :  300 :  008104.jpg\n",
      "1505 :  301 :  006287.jpg\n",
      "1506 :  302 :  000665.jpg\n",
      "1508 :  303 :  003431.jpg\n",
      "1511 :  304 :  004072.jpg\n",
      "1513 :  305 :  007169.jpg\n",
      "1515 :  306 :  008625.jpg\n",
      "1520 :  307 :  003123.jpg\n",
      "1523 :  308 :  001047.jpg\n",
      "1550 :  309 :  000856.jpg\n",
      "1556 :  310 :  005193.jpg\n",
      "1559 :  311 :  007569.jpg\n",
      "1561 :  312 :  007597.jpg\n",
      "1564 :  313 :  009263.jpg\n",
      "1574 :  314 :  000085.jpg\n",
      "1575 :  315 :  004733.jpg\n",
      "1585 :  316 :  009553.jpg\n",
      "1599 :  317 :  001505.jpg\n",
      "1600 :  318 :  000369.jpg\n",
      "1603 :  319 :  000414.jpg\n",
      "1605 :  320 :  001099.jpg\n",
      "1606 :  321 :  000418.jpg\n",
      "1617 :  322 :  009631.jpg\n",
      "1619 :  323 :  006406.jpg\n",
      "1627 :  324 :  006662.jpg\n",
      "1632 :  325 :  004813.jpg\n",
      "1655 :  326 :  000706.jpg\n",
      "1669 :  327 :  002654.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1678 :  328 :  006546.jpg\n",
      "1692 :  329 :  008440.jpg\n",
      "1693 :  330 :  004045.jpg\n",
      "1698 :  331 :  009356.jpg\n",
      "1706 :  332 :  007806.jpg\n",
      "1709 :  333 :  000216.jpg\n",
      "1722 :  334 :  003144.jpg\n",
      "1729 :  335 :  001086.jpg\n",
      "1730 :  336 :  005294.jpg\n",
      "1734 :  337 :  002777.jpg\n",
      "1737 :  338 :  007335.jpg\n",
      "1744 :  339 :  006830.jpg\n",
      "1745 :  340 :  009929.jpg\n",
      "1748 :  341 :  006142.jpg\n",
      "1756 :  342 :  003488.jpg\n",
      "1765 :  343 :  005323.jpg\n",
      "1778 :  344 :  006051.jpg\n",
      "1779 :  345 :  000008.jpg\n",
      "1780 :  346 :  003574.jpg\n",
      "1788 :  347 :  008198.jpg\n",
      "1789 :  348 :  008563.jpg\n",
      "1791 :  349 :  003221.jpg\n",
      "1796 :  350 :  001814.jpg\n",
      "1799 :  351 :  009154.jpg\n",
      "1802 :  352 :  004032.jpg\n",
      "1804 :  353 :  006923.jpg\n",
      "1811 :  354 :  009222.jpg\n",
      "1833 :  355 :  009802.jpg\n",
      "1840 :  356 :  001783.jpg\n",
      "1843 :  357 :  008153.jpg\n",
      "1847 :  358 :  005226.jpg\n",
      "1853 :  359 :  005313.jpg\n",
      "1856 :  360 :  002301.jpg\n",
      "1857 :  361 :  006592.jpg\n",
      "1868 :  362 :  000817.jpg\n",
      "1872 :  363 :  005216.jpg\n",
      "1877 :  364 :  004645.jpg\n",
      "1879 :  365 :  007202.jpg\n",
      "1894 :  366 :  000762.jpg\n",
      "1895 :  367 :  004752.jpg\n",
      "1902 :  368 :  003251.jpg\n",
      "1904 :  369 :  003600.jpg\n",
      "1912 :  370 :  000606.jpg\n",
      "1916 :  371 :  000151.jpg\n",
      "1919 :  372 :  000510.jpg\n",
      "1921 :  373 :  009645.jpg\n",
      "1922 :  374 :  005238.jpg\n",
      "1936 :  375 :  007985.jpg\n",
      "1943 :  376 :  005596.jpg\n",
      "1962 :  377 :  001585.jpg\n",
      "1987 :  378 :  009930.jpg\n",
      "1989 :  379 :  008363.jpg\n",
      "1991 :  380 :  008184.jpg\n",
      "2006 :  381 :  006126.jpg\n",
      "2027 :  382 :  006169.jpg\n",
      "2039 :  383 :  006056.jpg\n",
      "2045 :  384 :  008963.jpg\n",
      "2048 :  385 :  009651.jpg\n",
      "2051 :  386 :  004668.jpg\n",
      "2063 :  387 :  000611.jpg\n",
      "2065 :  388 :  000003.jpg\n",
      "2067 :  389 :  007404.jpg\n",
      "2070 :  390 :  008394.jpg\n",
      "2072 :  391 :  000749.jpg\n",
      "2079 :  392 :  008520.jpg\n",
      "2085 :  393 :  001377.jpg\n",
      "2090 :  394 :  002787.jpg\n",
      "2095 :  395 :  005555.jpg\n",
      "2097 :  396 :  005936.jpg\n",
      "2099 :  397 :  001568.jpg\n",
      "2101 :  398 :  008803.jpg\n",
      "2102 :  399 :  004078.jpg\n",
      "2103 :  400 :  003823.jpg\n",
      "2106 :  401 :  000280.jpg\n",
      "2108 :  402 :  005106.jpg\n",
      "2109 :  403 :  004497.jpg\n",
      "2110 :  404 :  007096.jpg\n",
      "2129 :  405 :  009164.jpg\n",
      "2136 :  406 :  009095.jpg\n",
      "2140 :  407 :  006086.jpg\n",
      "2145 :  408 :  001046.jpg\n",
      "2152 :  409 :  005623.jpg\n",
      "2163 :  410 :  001631.jpg\n",
      "2174 :  411 :  004268.jpg\n",
      "2180 :  412 :  003950.jpg\n",
      "2185 :  413 :  002026.jpg\n",
      "2187 :  414 :  006745.jpg\n",
      "2198 :  415 :  004533.jpg\n",
      "2199 :  416 :  007164.jpg\n",
      "2204 :  417 :  000116.jpg\n",
      "2206 :  418 :  007456.jpg\n",
      "2214 :  419 :  009928.jpg\n",
      "2216 :  420 :  001513.jpg\n",
      "2220 :  421 :  002982.jpg\n",
      "2227 :  422 :  005428.jpg\n",
      "2228 :  423 :  006452.jpg\n",
      "2238 :  424 :  006057.jpg\n",
      "2239 :  425 :  008779.jpg\n",
      "2241 :  426 :  009690.jpg\n",
      "2247 :  427 :  002065.jpg\n",
      "2251 :  428 :  007393.jpg\n",
      "2253 :  429 :  005180.jpg\n",
      "2260 :  430 :  000452.jpg\n",
      "2261 :  431 :  004906.jpg\n",
      "2263 :  432 :  009030.jpg\n",
      "2287 :  433 :  006795.jpg\n",
      "2307 :  434 :  001126.jpg\n",
      "2311 :  435 :  007567.jpg\n",
      "2316 :  436 :  002928.jpg\n",
      "2318 :  437 :  004633.jpg\n",
      "2338 :  438 :  001914.jpg\n",
      "2340 :  439 :  005498.jpg\n",
      "2349 :  440 :  007862.jpg\n",
      "2350 :  441 :  008673.jpg\n",
      "2356 :  442 :  002661.jpg\n",
      "2358 :  443 :  001602.jpg\n",
      "2375 :  444 :  004250.jpg\n",
      "2379 :  445 :  004363.jpg\n",
      "2381 :  446 :  008954.jpg\n",
      "2384 :  447 :  009914.jpg\n",
      "2393 :  448 :  002843.jpg\n",
      "2397 :  449 :  009901.jpg\n",
      "2402 :  450 :  003943.jpg\n",
      "2412 :  451 :  000067.jpg\n",
      "2421 :  452 :  002629.jpg\n",
      "2424 :  453 :  008729.jpg\n",
      "2425 :  454 :  009431.jpg\n",
      "2426 :  455 :  007532.jpg\n",
      "2429 :  456 :  000953.jpg\n",
      "2432 :  457 :  006380.jpg\n",
      "2442 :  458 :  007067.jpg\n",
      "2451 :  459 :  004477.jpg\n",
      "2457 :  460 :  007406.jpg\n",
      "2460 :  461 :  004572.jpg\n",
      "2467 :  462 :  008897.jpg\n",
      "2470 :  463 :  004680.jpg\n",
      "2471 :  464 :  002353.jpg\n",
      "2475 :  465 :  003448.jpg\n",
      "2482 :  466 :  005174.jpg\n",
      "2483 :  467 :  006646.jpg\n",
      "2499 :  468 :  009595.jpg\n",
      "2502 :  469 :  006232.jpg\n",
      "2505 :  470 :  001534.jpg\n",
      "2507 :  471 :  007315.jpg\n",
      "2509 :  472 :  007783.jpg\n",
      "2511 :  473 :  004599.jpg\n",
      "2519 :  474 :  000691.jpg\n",
      "2523 :  475 :  001990.jpg\n",
      "2531 :  476 :  002665.jpg\n",
      "2532 :  477 :  002711.jpg\n",
      "2533 :  478 :  004988.jpg\n",
      "2534 :  479 :  001811.jpg\n",
      "2538 :  480 :  009633.jpg\n",
      "2539 :  481 :  007026.jpg\n",
      "2543 :  482 :  001599.jpg\n",
      "2548 :  483 :  008050.jpg\n",
      "2550 :  484 :  007364.jpg\n",
      "2552 :  485 :  009876.jpg\n",
      "2554 :  486 :  002846.jpg\n",
      "2572 :  487 :  001189.jpg\n",
      "2574 :  488 :  005727.jpg\n",
      "2575 :  489 :  008600.jpg\n",
      "2578 :  490 :  006422.jpg\n",
      "2582 :  491 :  006093.jpg\n",
      "2591 :  492 :  000128.jpg\n",
      "2595 :  493 :  005844.jpg\n",
      "2596 :  494 :  007644.jpg\n",
      "2604 :  495 :  009741.jpg\n",
      "2607 :  496 :  009871.jpg\n",
      "2608 :  497 :  004559.jpg\n",
      "2616 :  498 :  001600.jpg\n",
      "2632 :  499 :  002822.jpg\n",
      "2637 :  500 :  006713.jpg\n",
      "2647 :  501 :  001021.jpg\n",
      "2661 :  502 :  001994.jpg\n",
      "2662 :  503 :  007255.jpg\n",
      "2667 :  504 :  000339.jpg\n",
      "2668 :  505 :  003490.jpg\n",
      "2676 :  506 :  008908.jpg\n",
      "2678 :  507 :  001295.jpg\n",
      "2680 :  508 :  002950.jpg\n",
      "2684 :  509 :  004887.jpg\n",
      "2695 :  510 :  002949.jpg\n",
      "2696 :  511 :  007708.jpg\n",
      "2699 :  512 :  008591.jpg\n",
      "2700 :  513 :  009626.jpg\n",
      "2702 :  514 :  008178.jpg\n",
      "2703 :  515 :  006003.jpg\n",
      "2706 :  516 :  008740.jpg\n",
      "2707 :  517 :  003141.jpg\n",
      "2709 :  518 :  008287.jpg\n",
      "2716 :  519 :  003853.jpg\n",
      "2718 :  520 :  002712.jpg\n",
      "2720 :  521 :  007366.jpg\n",
      "2721 :  522 :  007157.jpg\n",
      "2722 :  523 :  003650.jpg\n",
      "2725 :  524 :  002617.jpg\n",
      "2734 :  525 :  002974.jpg\n",
      "2736 :  526 :  006732.jpg\n",
      "2745 :  527 :  006533.jpg\n",
      "2760 :  528 :  005766.jpg\n",
      "2764 :  529 :  004661.jpg\n",
      "2773 :  530 :  003347.jpg\n",
      "2774 :  531 :  001080.jpg\n",
      "2777 :  532 :  006721.jpg\n",
      "2782 :  533 :  008705.jpg\n",
      "2788 :  534 :  003867.jpg\n",
      "2806 :  535 :  005661.jpg\n",
      "2807 :  536 :  008407.jpg\n",
      "2810 :  537 :  003345.jpg\n",
      "2812 :  538 :  009582.jpg\n",
      "2813 :  539 :  001762.jpg\n",
      "2814 :  540 :  006402.jpg\n",
      "2815 :  541 :  000217.jpg\n",
      "2817 :  542 :  000976.jpg\n",
      "2820 :  543 :  007937.jpg\n",
      "2822 :  544 :  006624.jpg\n",
      "2829 :  545 :  009297.jpg\n",
      "2834 :  546 :  000178.jpg\n",
      "2838 :  547 :  008486.jpg\n",
      "2842 :  548 :  002087.jpg\n",
      "2843 :  549 :  006390.jpg\n",
      "2844 :  550 :  006984.jpg\n",
      "2852 :  551 :  005276.jpg\n",
      "2856 :  552 :  002604.jpg\n",
      "2865 :  553 :  007598.jpg\n",
      "2866 :  554 :  008045.jpg\n",
      "2868 :  555 :  001868.jpg\n",
      "2873 :  556 :  003115.jpg\n",
      "2878 :  557 :  001163.jpg\n",
      "2883 :  558 :  004119.jpg\n",
      "2890 :  559 :  008414.jpg\n",
      "2896 :  560 :  000692.jpg\n",
      "2899 :  561 :  004698.jpg\n",
      "2903 :  562 :  002769.jpg\n",
      "2912 :  563 :  000185.jpg\n",
      "2915 :  564 :  008016.jpg\n",
      "2918 :  565 :  005726.jpg\n",
      "2920 :  566 :  007717.jpg\n",
      "2922 :  567 :  001246.jpg\n",
      "2928 :  568 :  006500.jpg\n",
      "2933 :  569 :  002905.jpg\n",
      "2937 :  570 :  006541.jpg\n",
      "2941 :  571 :  000377.jpg\n",
      "2951 :  572 :  008957.jpg\n",
      "2956 :  573 :  008671.jpg\n",
      "2958 :  574 :  009292.jpg\n",
      "2959 :  575 :  009109.jpg\n",
      "2962 :  576 :  005597.jpg\n",
      "2970 :  577 :  006024.jpg\n",
      "2974 :  578 :  009010.jpg\n",
      "2985 :  579 :  001489.jpg\n",
      "2986 :  580 :  005392.jpg\n",
      "2987 :  581 :  005941.jpg\n",
      "2992 :  582 :  004040.jpg\n",
      "2993 :  583 :  006461.jpg\n",
      "2994 :  584 :  008330.jpg\n",
      "3001 :  585 :  008754.jpg\n",
      "3003 :  586 :  005362.jpg\n",
      "3010 :  587 :  004314.jpg\n",
      "3012 :  588 :  004109.jpg\n",
      "3015 :  589 :  008734.jpg\n",
      "3024 :  590 :  008697.jpg\n",
      "3029 :  591 :  001354.jpg\n",
      "3030 :  592 :  009798.jpg\n",
      "3031 :  593 :  007837.jpg\n",
      "3033 :  594 :  003906.jpg\n",
      "3034 :  595 :  009514.jpg\n",
      "3035 :  596 :  008257.jpg\n",
      "3047 :  597 :  000097.jpg\n",
      "3052 :  598 :  008134.jpg\n",
      "3057 :  599 :  005412.jpg\n",
      "3060 :  600 :  007401.jpg\n",
      "3063 :  601 :  008156.jpg\n",
      "3064 :  602 :  008196.jpg\n",
      "3069 :  603 :  001812.jpg\n",
      "3071 :  604 :  003010.jpg\n",
      "3077 :  605 :  008458.jpg\n",
      "3091 :  606 :  004567.jpg\n",
      "3097 :  607 :  007220.jpg\n",
      "3101 :  608 :  000196.jpg\n",
      "3102 :  609 :  009662.jpg\n",
      "3108 :  610 :  000517.jpg\n",
      "3114 :  611 :  004546.jpg\n",
      "3125 :  612 :  001674.jpg\n",
      "3130 :  613 :  000940.jpg\n",
      "3132 :  614 :  008941.jpg\n",
      "3135 :  615 :  009083.jpg\n",
      "3139 :  616 :  002297.jpg\n",
      "3148 :  617 :  007628.jpg\n",
      "3149 :  618 :  002703.jpg\n",
      "3151 :  619 :  001975.jpg\n",
      "3153 :  620 :  000938.jpg\n",
      "3154 :  621 :  004918.jpg\n",
      "3155 :  622 :  005926.jpg\n",
      "3167 :  623 :  000366.jpg\n",
      "3168 :  624 :  006577.jpg\n",
      "3173 :  625 :  009635.jpg\n",
      "3179 :  626 :  007286.jpg\n",
      "3186 :  627 :  008073.jpg\n",
      "3189 :  628 :  007367.jpg\n",
      "3193 :  629 :  001244.jpg\n",
      "3198 :  630 :  005754.jpg\n",
      "3204 :  631 :  001373.jpg\n",
      "3205 :  632 :  004311.jpg\n",
      "3206 :  633 :  000600.jpg\n",
      "3213 :  634 :  008481.jpg\n",
      "3214 :  635 :  000668.jpg\n",
      "3216 :  636 :  009632.jpg\n",
      "3220 :  637 :  007237.jpg\n",
      "3223 :  638 :  008686.jpg\n",
      "3230 :  639 :  009815.jpg\n",
      "3233 :  640 :  001992.jpg\n",
      "3235 :  641 :  003322.jpg\n",
      "3246 :  642 :  001035.jpg\n",
      "3248 :  643 :  001621.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3250 :  644 :  006954.jpg\n",
      "3254 :  645 :  001629.jpg\n",
      "3255 :  646 :  008791.jpg\n",
      "3266 :  647 :  009853.jpg\n",
      "3269 :  648 :  009435.jpg\n",
      "3270 :  649 :  007832.jpg\n",
      "3277 :  650 :  003819.jpg\n",
      "3282 :  651 :  009824.jpg\n",
      "3284 :  652 :  002207.jpg\n",
      "3289 :  653 :  003776.jpg\n",
      "3292 :  654 :  004712.jpg\n",
      "3293 :  655 :  006888.jpg\n",
      "3297 :  656 :  002560.jpg\n",
      "3301 :  657 :  002707.jpg\n",
      "3306 :  658 :  007785.jpg\n",
      "3314 :  659 :  008751.jpg\n",
      "3320 :  660 :  005279.jpg\n",
      "3321 :  661 :  005976.jpg\n",
      "3333 :  662 :  006274.jpg\n",
      "3342 :  663 :  006752.jpg\n",
      "3352 :  664 :  003067.jpg\n",
      "3358 :  665 :  000327.jpg\n",
      "3360 :  666 :  009478.jpg\n",
      "3370 :  667 :  006895.jpg\n",
      "3385 :  668 :  001025.jpg\n",
      "3397 :  669 :  006195.jpg\n",
      "3419 :  670 :  001105.jpg\n",
      "3423 :  671 :  002809.jpg\n",
      "3428 :  672 :  000652.jpg\n",
      "3437 :  673 :  001996.jpg\n",
      "3439 :  674 :  009329.jpg\n",
      "3440 :  675 :  007391.jpg\n",
      "3446 :  676 :  001720.jpg\n",
      "3451 :  677 :  005464.jpg\n",
      "3457 :  678 :  005491.jpg\n",
      "3467 :  679 :  003049.jpg\n",
      "3472 :  680 :  000157.jpg\n",
      "3485 :  681 :  003707.jpg\n",
      "3493 :  682 :  005402.jpg\n",
      "3496 :  683 :  003297.jpg\n",
      "3500 :  684 :  006145.jpg\n",
      "3503 :  685 :  009452.jpg\n",
      "3506 :  686 :  000059.jpg\n",
      "3512 :  687 :  004819.jpg\n",
      "3513 :  688 :  003206.jpg\n",
      "3514 :  689 :  009167.jpg\n",
      "3519 :  690 :  006226.jpg\n",
      "3521 :  691 :  007817.jpg\n",
      "3551 :  692 :  001884.jpg\n",
      "3553 :  693 :  004056.jpg\n",
      "3554 :  694 :  002081.jpg\n",
      "3557 :  695 :  002240.jpg\n",
      "3564 :  696 :  000905.jpg\n",
      "3568 :  697 :  000260.jpg\n",
      "3575 :  698 :  001670.jpg\n",
      "3582 :  699 :  009521.jpg\n",
      "3586 :  700 :  006072.jpg\n",
      "3591 :  701 :  007472.jpg\n",
      "3600 :  702 :  008887.jpg\n",
      "3601 :  703 :  007494.jpg\n",
      "3602 :  704 :  006010.jpg\n",
      "3605 :  705 :  002754.jpg\n",
      "3613 :  706 :  009444.jpg\n",
      "3615 :  707 :  003445.jpg\n",
      "3617 :  708 :  009262.jpg\n",
      "3623 :  709 :  007301.jpg\n",
      "3629 :  710 :  000335.jpg\n",
      "3644 :  711 :  007778.jpg\n",
      "3652 :  712 :  004128.jpg\n",
      "3674 :  713 :  005096.jpg\n",
      "3684 :  714 :  003541.jpg\n",
      "3686 :  715 :  003738.jpg\n",
      "3687 :  716 :  003323.jpg\n",
      "3698 :  717 :  007989.jpg\n",
      "3699 :  718 :  005898.jpg\n",
      "3701 :  719 :  002951.jpg\n",
      "3705 :  720 :  005234.jpg\n",
      "3707 :  721 :  007711.jpg\n",
      "3710 :  722 :  004449.jpg\n",
      "3713 :  723 :  007648.jpg\n",
      "3715 :  724 :  001929.jpg\n",
      "3728 :  725 :  004556.jpg\n",
      "3732 :  726 :  001805.jpg\n",
      "3733 :  727 :  006477.jpg\n",
      "3743 :  728 :  007504.jpg\n",
      "3747 :  729 :  000618.jpg\n",
      "3762 :  730 :  000886.jpg\n",
      "3771 :  731 :  007225.jpg\n",
      "3774 :  732 :  007257.jpg\n",
      "3776 :  733 :  002506.jpg\n",
      "3782 :  734 :  000299.jpg\n",
      "3793 :  735 :  003881.jpg\n",
      "3796 :  736 :  001167.jpg\n",
      "3803 :  737 :  001957.jpg\n",
      "3818 :  738 :  007756.jpg\n",
      "3823 :  739 :  008382.jpg\n",
      "3832 :  740 :  004422.jpg\n",
      "3835 :  741 :  005442.jpg\n",
      "3837 :  742 :  009118.jpg\n",
      "3840 :  743 :  000247.jpg\n",
      "3842 :  744 :  005266.jpg\n",
      "3843 :  745 :  007816.jpg\n",
      "3844 :  746 :  009606.jpg\n",
      "3848 :  747 :  005060.jpg\n",
      "3852 :  748 :  001773.jpg\n",
      "3870 :  749 :  003825.jpg\n",
      "3871 :  750 :  003381.jpg\n",
      "3875 :  751 :  000202.jpg\n",
      "3878 :  752 :  008347.jpg\n",
      "3883 :  753 :  007739.jpg\n",
      "3894 :  754 :  007442.jpg\n",
      "3901 :  755 :  009630.jpg\n",
      "3902 :  756 :  002650.jpg\n",
      "3904 :  757 :  001429.jpg\n",
      "3907 :  758 :  008094.jpg\n",
      "3911 :  759 :  006676.jpg\n",
      "3914 :  760 :  002945.jpg\n",
      "3915 :  761 :  003494.jpg\n",
      "3918 :  762 :  007835.jpg\n",
      "3921 :  763 :  005627.jpg\n",
      "3925 :  764 :  000547.jpg\n",
      "3926 :  765 :  008493.jpg\n",
      "3927 :  766 :  007492.jpg\n",
      "3938 :  767 :  004398.jpg\n",
      "3942 :  768 :  009225.jpg\n",
      "3943 :  769 :  002894.jpg\n",
      "3953 :  770 :  000974.jpg\n",
      "3960 :  771 :  003725.jpg\n",
      "3967 :  772 :  004162.jpg\n",
      "3972 :  773 :  008459.jpg\n",
      "3976 :  774 :  003902.jpg\n",
      "3977 :  775 :  004068.jpg\n",
      "3980 :  776 :  001340.jpg\n",
      "3984 :  777 :  008041.jpg\n",
      "3986 :  778 :  003037.jpg\n",
      "3988 :  779 :  008902.jpg\n",
      "3998 :  780 :  004476.jpg\n",
      "4005 :  781 :  007870.jpg\n",
      "4021 :  782 :  006705.jpg\n",
      "4035 :  783 :  004153.jpg\n",
      "4037 :  784 :  000084.jpg\n",
      "4041 :  785 :  000968.jpg\n",
      "4048 :  786 :  004862.jpg\n",
      "4049 :  787 :  000487.jpg\n",
      "4052 :  788 :  001179.jpg\n",
      "4058 :  789 :  001276.jpg\n",
      "4062 :  790 :  009689.jpg\n",
      "4069 :  791 :  006630.jpg\n",
      "4070 :  792 :  004791.jpg\n",
      "4072 :  793 :  002619.jpg\n",
      "4073 :  794 :  001652.jpg\n",
      "4076 :  795 :  001351.jpg\n",
      "4078 :  796 :  001302.jpg\n",
      "4079 :  797 :  003526.jpg\n",
      "4082 :  798 :  004278.jpg\n",
      "4102 :  799 :  000006.jpg\n",
      "4103 :  800 :  001551.jpg\n",
      "4104 :  801 :  004355.jpg\n",
      "4107 :  802 :  004620.jpg\n",
      "4108 :  803 :  007508.jpg\n",
      "4112 :  804 :  003139.jpg\n",
      "4113 :  805 :  004166.jpg\n",
      "4118 :  806 :  009919.jpg\n",
      "4131 :  807 :  002414.jpg\n",
      "4139 :  808 :  003676.jpg\n",
      "4144 :  809 :  005589.jpg\n",
      "4149 :  810 :  003446.jpg\n",
      "4150 :  811 :  002663.jpg\n",
      "4153 :  812 :  005635.jpg\n",
      "4157 :  813 :  008554.jpg\n",
      "4168 :  814 :  005458.jpg\n",
      "4169 :  815 :  004118.jpg\n",
      "4176 :  816 :  001177.jpg\n",
      "4177 :  817 :  001261.jpg\n",
      "4178 :  818 :  003944.jpg\n",
      "4190 :  819 :  004199.jpg\n",
      "4192 :  820 :  002788.jpg\n",
      "4194 :  821 :  004180.jpg\n",
      "4211 :  822 :  002429.jpg\n",
      "4213 :  823 :  007825.jpg\n",
      "4216 :  824 :  008543.jpg\n",
      "4219 :  825 :  006050.jpg\n",
      "4221 :  826 :  002581.jpg\n",
      "4223 :  827 :  000226.jpg\n",
      "4235 :  828 :  008182.jpg\n",
      "4237 :  829 :  006792.jpg\n",
      "4238 :  830 :  004042.jpg\n",
      "4248 :  831 :  000981.jpg\n",
      "4267 :  832 :  003160.jpg\n",
      "4268 :  833 :  002773.jpg\n",
      "4275 :  834 :  002322.jpg\n",
      "4277 :  835 :  009294.jpg\n",
      "4281 :  836 :  000243.jpg\n",
      "4285 :  837 :  007240.jpg\n",
      "4288 :  838 :  007012.jpg\n",
      "4290 :  839 :  008233.jpg\n",
      "4308 :  840 :  005633.jpg\n",
      "4311 :  841 :  001912.jpg\n",
      "4321 :  842 :  002159.jpg\n",
      "4330 :  843 :  003931.jpg\n",
      "4331 :  844 :  001349.jpg\n",
      "4346 :  845 :  007110.jpg\n",
      "4349 :  846 :  009055.jpg\n",
      "4357 :  847 :  000659.jpg\n",
      "4361 :  848 :  001419.jpg\n",
      "4366 :  849 :  003438.jpg\n",
      "4367 :  850 :  003249.jpg\n",
      "4369 :  851 :  005575.jpg\n",
      "4375 :  852 :  006307.jpg\n",
      "4376 :  853 :  006347.jpg\n",
      "4377 :  854 :  007384.jpg\n",
      "4378 :  855 :  003076.jpg\n",
      "4383 :  856 :  002389.jpg\n",
      "4387 :  857 :  004061.jpg\n",
      "4391 :  858 :  006152.jpg\n",
      "4397 :  859 :  005767.jpg\n",
      "4399 :  860 :  001138.jpg\n",
      "4417 :  861 :  009487.jpg\n",
      "4420 :  862 :  009511.jpg\n",
      "4424 :  863 :  006451.jpg\n",
      "4429 :  864 :  008844.jpg\n",
      "4431 :  865 :  009660.jpg\n",
      "4435 :  866 :  001850.jpg\n",
      "4441 :  867 :  003319.jpg\n",
      "4449 :  868 :  005934.jpg\n",
      "4452 :  869 :  000737.jpg\n",
      "4467 :  870 :  001848.jpg\n",
      "4472 :  871 :  004881.jpg\n",
      "4489 :  872 :  003697.jpg\n",
      "4502 :  873 :  009313.jpg\n",
      "4503 :  874 :  006457.jpg\n",
      "4504 :  875 :  007744.jpg\n",
      "4514 :  876 :  002014.jpg\n",
      "4518 :  877 :  009838.jpg\n",
      "4535 :  878 :  000641.jpg\n",
      "4545 :  879 :  008626.jpg\n",
      "4549 :  880 :  000893.jpg\n",
      "4551 :  881 :  005665.jpg\n",
      "4559 :  882 :  009257.jpg\n",
      "4561 :  883 :  001023.jpg\n",
      "4574 :  884 :  002862.jpg\n",
      "4584 :  885 :  008583.jpg\n",
      "4593 :  886 :  002189.jpg\n",
      "4601 :  887 :  004720.jpg\n",
      "4608 :  888 :  005376.jpg\n",
      "4621 :  889 :  003928.jpg\n",
      "4622 :  890 :  002538.jpg\n",
      "4653 :  891 :  002052.jpg\n",
      "4655 :  892 :  002141.jpg\n",
      "4671 :  893 :  006271.jpg\n",
      "4673 :  894 :  009547.jpg\n",
      "4675 :  895 :  005545.jpg\n",
      "4676 :  896 :  003314.jpg\n",
      "4686 :  897 :  002089.jpg\n",
      "4716 :  898 :  008528.jpg\n",
      "4723 :  899 :  001139.jpg\n",
      "4734 :  900 :  003309.jpg\n",
      "4742 :  901 :  006946.jpg\n",
      "4745 :  902 :  002610.jpg\n",
      "4753 :  903 :  004784.jpg\n",
      "4756 :  904 :  004721.jpg\n",
      "4758 :  905 :  002107.jpg\n",
      "4760 :  906 :  002656.jpg\n",
      "4767 :  907 :  005858.jpg\n",
      "4770 :  908 :  004064.jpg\n",
      "4773 :  909 :  009310.jpg\n",
      "4774 :  910 :  004880.jpg\n",
      "4776 :  911 :  002614.jpg\n",
      "4781 :  912 :  001223.jpg\n",
      "4785 :  913 :  008938.jpg\n",
      "4797 :  914 :  004366.jpg\n",
      "4798 :  915 :  007319.jpg\n",
      "4800 :  916 :  007155.jpg\n",
      "4803 :  917 :  004134.jpg\n",
      "4817 :  918 :  005034.jpg\n",
      "4818 :  919 :  005295.jpg\n",
      "4820 :  920 :  005184.jpg\n",
      "4824 :  921 :  006774.jpg\n",
      "4825 :  922 :  000825.jpg\n",
      "4830 :  923 :  002575.jpg\n",
      "4834 :  924 :  007502.jpg\n",
      "4849 :  925 :  002552.jpg\n",
      "4851 :  926 :  005703.jpg\n",
      "4853 :  927 :  007995.jpg\n",
      "4854 :  928 :  000447.jpg\n",
      "4855 :  929 :  000346.jpg\n",
      "4856 :  930 :  008864.jpg\n",
      "4859 :  931 :  008055.jpg\n",
      "4863 :  932 :  006644.jpg\n",
      "4870 :  933 :  003643.jpg\n",
      "4882 :  934 :  000595.jpg\n",
      "4892 :  935 :  001609.jpg\n",
      "4894 :  936 :  006081.jpg\n",
      "4895 :  937 :  008555.jpg\n",
      "4896 :  938 :  004892.jpg\n",
      "4899 :  939 :  003582.jpg\n",
      "4914 :  940 :  007352.jpg\n",
      "4937 :  941 :  009854.jpg\n",
      "4943 :  942 :  009840.jpg\n",
      "4946 :  943 :  004491.jpg\n",
      "4947 :  944 :  006663.jpg\n",
      "4949 :  945 :  006393.jpg\n",
      "4950 :  946 :  004236.jpg\n",
      "Average Precision for class aeroplane is 1.0\n",
      "Correctly predicted images for class aeroplane are [u'009292.jpg', u'002198.jpg', u'001850.jpg', u'002619.jpg', u'007985.jpg', u'002246.jpg', u'008825.jpg', u'008352.jpg', u'008986.jpg', u'002754.jpg', u'001885.jpg', u'006888.jpg', u'008554.jpg', u'007155.jpg', u'003494.jpg', u'009835.jpg', u'007825.jpg', u'009329.jpg', u'000067.jpg', u'007157.jpg', u'007837.jpg', u'001547.jpg', u'000696.jpg', u'001912.jpg', u'007164.jpg', u'004314.jpg', u'007096.jpg']\n",
      "Average Precision for class bottle is 0.162679425837\n",
      "Correctly predicted images for class bottle are [u'005002.jpg', u'009257.jpg', u'002231.jpg', u'007500.jpg', u'008925.jpg', u'006390.jpg', u'003010.jpg', u'004134.jpg', u'000369.jpg', u'006500.jpg', u'008627.jpg', u'004068.jpg', u'001295.jpg', u'002429.jpg', u'001429.jpg', u'002950.jpg', u'001925.jpg', u'001086.jpg', u'001925.jpg', u'004055.jpg', u'009431.jpg', u'004476.jpg', u'004355.jpg', u'007456.jpg', u'001652.jpg', u'003431.jpg', u'001925.jpg', u'004236.jpg', u'000346.jpg', u'005491.jpg', u'002711.jpg', u'004236.jpg', u'006500.jpg', u'001925.jpg', u'000447.jpg', u'008287.jpg', u'001652.jpg', u'005635.jpg', u'009547.jpg', u'007744.jpg']\n",
      "Average Precision for class chair is 0.0479192938209\n",
      "Correctly predicted images for class chair are [u'004262.jpg', u'003502.jpg', u'003643.jpg', u'008991.jpg', u'009677.jpg', u'006297.jpg', u'000856.jpg', u'008922.jpg', u'003345.jpg', u'009635.jpg', u'009854.jpg', u'001169.jpg', u'004319.jpg', u'005661.jpg', u'004820.jpg', u'005464.jpg', u'002974.jpg', u'002724.jpg', u'002850.jpg', u'001276.jpg', u'007896.jpg', u'003738.jpg', u'005734.jpg', u'007348.jpg', u'008113.jpg', u'009854.jpg', u'005734.jpg', u'000181.jpg', u'004695.jpg', u'005193.jpg', u'005295.jpg', u'006792.jpg', u'000510.jpg', u'009662.jpg', u'005665.jpg', u'000595.jpg', u'000084.jpg', u'003323.jpg', u'005180.jpg', u'005184.jpg', u'001163.jpg', u'006142.jpg', u'007255.jpg', u'006662.jpg', u'002389.jpg', u'003345.jpg', u'001189.jpg', u'007442.jpg', u'005589.jpg', u'009606.jpg', u'005597.jpg', u'009690.jpg', u'001551.jpg', u'000385.jpg', u'004640.jpg', u'007319.jpg', u'008113.jpg', u'007240.jpg', u'008941.jpg', u'000510.jpg', u'001138.jpg', u'003502.jpg', u'002930.jpg', u'001189.jpg', u'006662.jpg', u'002650.jpg', u'002322.jpg', u'002930.jpg', u'000595.jpg', u'004813.jpg', u'005044.jpg', u'001139.jpg', u'003323.jpg', u'009853.jpg', u'005941.jpg', u'000595.jpg']\n",
      "MAP: 0.403532906553\n",
      "CPU times: user 10h 27min 10s, sys: 1h 7min 39s, total: 11h 34min 49s\n",
      "Wall time: 5h 57min 25s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.40353290655275126"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time test_two_layer(ssdResnet3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
